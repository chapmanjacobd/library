interactions:
- request:
    body: null
    headers:
      A-Im:
      - feed
      Accept:
      - application/atom+xml,application/rdf+xml,application/rss+xml,application/x-netcdf,application/xml;q=0.9,text/xml;q=0.2,*/*;q=0.1
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - close
      Host:
      - simonwillison.net
      User-Agent:
      - feedparser/6.0.10 +https://github.com/kurtmckee/feedparser/
    method: GET
    uri: https://simonwillison.net/atom/everything/
  response:
    body:
      string: "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<feed xml:lang=\"en-us\"
        xmlns=\"http://www.w3.org/2005/Atom\"><title>Simon Willison's Weblog</title><link
        href=\"http://simonwillison.net/\" rel=\"alternate\"/><link href=\"http://simonwillison.net/atom/everything/\"
        rel=\"self\"/><id>http://simonwillison.net/</id><updated>2023-07-16T05:55:54+00:00</updated><author><name>Simon
        Willison</name></author><entry><title>Weeknotes: Self-hosted language models
        with LLM plugins, a new Datasette tutorial, a dozen package releases, a dozen
        TILs</title><link href=\"http://simonwillison.net/2023/Jul/16/weeknotes/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-16T05:55:54+00:00</published><updated>2023-07-16T05:55:54+00:00</updated><id>http://simonwillison.net/2023/Jul/16/weeknotes/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;A lot of stuff to cover from the past two and
        a half weeks.&lt;/p&gt;\r\n&lt;h4&gt;LLM and self-hosted language model plugins&lt;/h4&gt;\r\n&lt;p&gt;My
        biggest project was the &lt;a href=\"https://simonwillison.net/2023/Jul/12/llm/\"&gt;new
        version of my LLM tool for interacting with Large Language Models&lt;/a&gt;.
        LLM now accepts plugins for adding alternative language models to the tool,
        meaning it's now applicable to more than just the OpenAI collection.&lt;/p&gt;\r\n&lt;p&gt;I
        figured out quite a few of the details of this while offline on a camping
        trip up in the Northern California redwoods, which forced the issue on figuring
        out how to work with LLMs that I could host on my own computer because I didn't
        have a connection to access the OpenAI APIs.&lt;/p&gt;\r\n&lt;p&gt;Comprehensive
        documentation is sorely lacking in the world of generative AI. I've decided
        to push back against that for LLM, so I spent a bunch of time working on an
        extremely comprehensive tutorial for writing a plugin that adds a new language
        model to the tool:&lt;/p&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;&lt;strong&gt;&lt;a
        href=\"https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html\"&gt;Writing
        a plugin to support a new model&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;p&gt;As
        part of researching this tutorial I finally figured out how to build a Python
        package using just a &lt;code&gt;pyproject.toml&lt;/code&gt; file, with no
        &lt;code&gt;setup.py&lt;/code&gt; or &lt;code&gt;setup.cfg&lt;/code&gt; or
        anything else like that. I wrote that up in detail in &lt;a href=\"https://til.simonwillison.net/python/pyproject\"&gt;Python
        packages with pyproject.toml and nothing else&lt;/a&gt;, and I've started
        using that pattern for all of my new Python packages.&lt;/p&gt;\r\n&lt;p&gt;LLM
        also now includes a Python API for interacting with models, which provides
        an abstraction that works the same for the OpenAI models and for other models
        (including self-hosted models) installed via plugins. Here's &lt;a href=\"https://llm.datasette.io/en/stable/python-api.html\"&gt;the
        documentation for that&lt;/a&gt; - it ends up looking like this:&lt;/p&gt;\r\n&lt;pre&gt;&lt;span
        class=\"pl-k\"&gt;import&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;llm&lt;/span&gt;\r\n\r\n&lt;span
        class=\"pl-s1\"&gt;model&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;llm&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;get_model&lt;/span&gt;(&lt;span
        class=\"pl-s\"&gt;\"gpt-3.5-turbo\"&lt;/span&gt;)\r\n&lt;span class=\"pl-s1\"&gt;model&lt;/span&gt;.&lt;span
        class=\"pl-s1\"&gt;key&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s\"&gt;'YOUR_API_KEY_HERE'&lt;/span&gt;\r\n&lt;span class=\"pl-s1\"&gt;response&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;model&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;prompt&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;\"Five surprising
        names for a pet pelican\"&lt;/span&gt;)\r\n&lt;span class=\"pl-k\"&gt;for&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;chunk&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;in&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;response&lt;/span&gt;:\r\n    &lt;span class=\"pl-en\"&gt;print&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;chunk&lt;/span&gt;, &lt;span class=\"pl-s1\"&gt;end&lt;/span&gt;&lt;span
        class=\"pl-c1\"&gt;=&lt;/span&gt;&lt;span class=\"pl-s\"&gt;\"\"&lt;/span&gt;)&lt;/pre&gt;\r\n&lt;p&gt;To
        use another model, just swap its name in for &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;.
        The self-hosted models provided by the &lt;a href=\"https://github.com/simonw/llm-gpt4all\"&gt;llm-gpt4all&lt;/a&gt;
        plugin work the same way:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;pip
        install llm-gpt4all&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Then:&lt;/p&gt;\r\n&lt;pre&gt;&lt;span
        class=\"pl-k\"&gt;import&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;llm&lt;/span&gt;\r\n\r\n&lt;span
        class=\"pl-s1\"&gt;model&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;llm&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;get_model&lt;/span&gt;(&lt;span
        class=\"pl-s\"&gt;\"ggml-vicuna-7b-1\"&lt;/span&gt;)\r\n&lt;span class=\"pl-s1\"&gt;response&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;model&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;prompt&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;\"Five surprising
        names for a pet pelican\"&lt;/span&gt;)\r\n&lt;span class=\"pl-c\"&gt;# You
        can do this instead of looping through the chunks:&lt;/span&gt;\r\n&lt;span
        class=\"pl-en\"&gt;print&lt;/span&gt;(&lt;span class=\"pl-s1\"&gt;response&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;text&lt;/span&gt;())&lt;/pre&gt;\r\n&lt;p&gt;I've released
        three plugins so far:&lt;/p&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;\r\n&lt;a href=\"https://github.com/simonw/llm-gpt4all\"&gt;llm-gpt4all&lt;/a&gt;
        with 17 self-hosted models from the &lt;a href=\"https://gpt4all.io/\"&gt;GPT4All&lt;/a&gt;
        project.&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a href=\"https://github.com/simonw/llm-palm\"&gt;llm-palm&lt;/a&gt;
        with Google's &lt;a href=\"https://blog.google/technology/ai/google-palm-2-ai-large-language-model/\"&gt;PaLM
        2&lt;/a&gt; language model, via their API.&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://github.com/simonw/llm-mpt30b\"&gt;llm-mpt30b&lt;/a&gt; providing
        the 19GB MPT-30B model, using &lt;a href=\"https://huggingface.co/TheBloke/mpt-30B-GGML\"&gt;TheBloke/mpt-30B-GGML&lt;/a&gt;.&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;p&gt;I'm
        looking forward to someone else &lt;a href=\"https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html\"&gt;following
        the tutorial&lt;/a&gt; and releasing their own plugin!&lt;/p&gt;\r\n&lt;h4&gt;A
        new tutorial: Data analysis with SQLite and Python&lt;/h4&gt;\r\n&lt;p&gt;I
        presented this as a 2hr45m tutorial at PyCon a few months ago. The video is
        now available, and I like to try to turn these kinds of things into more permanent
        documentation.&lt;/p&gt;\r\n&lt;p&gt;The Datasette website has &lt;a href=\"https://datasette.io/tutorials\"&gt;a
        growing collection of tutorials&lt;/a&gt;, and I decided to make that the
        final home for this one too.&lt;/p&gt;\r\n&lt;p&gt;&lt;a href=\"https://datasette.io/tutorials/data-analysis\"&gt;Data
        analysis with SQLite and Python&lt;/a&gt; now has the full 2hr45m video plus
        an improved version of the handout I used for the talk. The written material
        there there should also be valuable for people who don't want to spend nearly
        three hours watching the video!&lt;/p&gt;\r\n&lt;p&gt;As part of putting that
        page together I solved a problem I've been wanting to figure out for a long
        time: I figured out a way to build a custom Jinja block tag that looks like
        this:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-text-html-django\"&gt;&lt;pre&gt;&lt;span
        class=\"pl-e\"&gt;{%&lt;/span&gt; &lt;span class=\"pl-s\"&gt;markdown&lt;/span&gt;
        &lt;span class=\"pl-e\"&gt;%}&lt;/span&gt;\r\n# This will be rendered as markdown\r\n\r\n-
        Bulleted\r\n- List\r\n&lt;span class=\"pl-e\"&gt;{%&lt;/span&gt; &lt;span
        class=\"pl-s\"&gt;endmarkdown&lt;/span&gt; &lt;span class=\"pl-e\"&gt;%}&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;I
        released that in &lt;a href=\"https://github.com/simonw/datasette-render-markdown/releases/tag/2.2\"&gt;datasette-render-markdown
        2.2&lt;/a&gt;. I also wrote up a TIL on &lt;a href=\"https://til.simonwillison.net/jinja/custom-jinja-tags-with-attributes\"&gt;Custom
        Jinja template tags with attributes&lt;/a&gt; describing the pattern I used.&lt;/p&gt;\r\n&lt;p&gt;One
        bonus feature for that tutorial: I decided to drop in a nested table of contents,
        automatically derived from the HTML headers on the page.&lt;/p&gt;\r\n&lt;p&gt;&lt;img
        src=\"https://static.simonwillison.net/static/2023/toc.jpg\" alt=\" What you'll
        need python3 and pip Optional: GitHub Codespaces sqlite-utils Using the command-line
        tools to clean data Exploring data with Datasette Installing Datasette locally
        Try a database: legislators.db Install some plugins Learning SQL with Datasette
        Using sqlite-utils as a Python library, to import all the PEPs Enabling full-text
        search Publishing a database to Vercel Other publishing options Datasette
        Lite Loading SQLite, CSV and JSON data Installing plugins Further reading
        Advanced SQL Aggregations Subqueries CTEs JSON Window functions Baked Data
        Niche Museums and TILs Generating a newsletter with an Observable notebook
        More demos and further reading Fun demos SpatiaLite \" style=\"max-width:
        100%;\" /&gt;&lt;/p&gt;\r\n&lt;p&gt;I wrote the code for this entirely using
        the new ChatGPT Code Interpreter, which can write Python based on your description
        and, crucially, &lt;em&gt;execute it and see if it works&lt;/em&gt;.&lt;/p&gt;\r\n&lt;p&gt;Here's
        &lt;a href=\"https://chat.openai.com/share/e41efb6d-eae7-454d-9aa2-5284683ba9f9\"&gt;my
        ChatGPT transcript&lt;/a&gt; showing how I built the feature.&lt;/p&gt;\r\n&lt;p&gt;I've
        been using ChatGPT Code Interpreter for a few months now, and I'm completely
        hooked: I think it's the most interesting thing in the whole AI space at the
        moment.&lt;/p&gt;\r\n&lt;p&gt;I participated in a &lt;a href=\"https://www.latent.space/p/code-interpreter\"&gt;Code
        Interpreter Latent Space&lt;/a&gt; episode to talk about it, which ended up
        drawing 17,000 listeners on Twitter Spaces and is now also available as a
        podcast episode, neatly edited together by swyx.&lt;/p&gt;\r\n&lt;h4&gt;Symbex
        --check and --rexec&lt;/h4&gt;\r\n&lt;p&gt;&lt;a href=\"https://github.com/simonw/symbex\"&gt;Symbex&lt;/a&gt;
        is my Python CLI tool for quickly finding Python functions and classes and
        outputting either the full code or just the signature of the matching symbol.
        I first &lt;a href=\"https://simonwillison.net/2023/Jun/18/symbex/\"&gt;wrote
        about that here&lt;/a&gt;.&lt;/p&gt;\r\n&lt;p&gt;&lt;a href=\"https://github.com/simonw/symbex/releases/tag/1.1\"&gt;symbex
        1.1&lt;/a&gt; adds two new features.&lt;/p&gt;\r\n&lt;div class=\"highlight
        highlight-source-shell\"&gt;&lt;pre&gt;symbex --function --undocumented --check&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;This
        new &lt;code&gt;--check&lt;/code&gt; mode is designed to run in Continuous
        Integration environments. If it finds any symbols matching the filters (in
        this case functions that are missing their docstring) it returns a non-zero
        exit code, which will fail the CI step.&lt;/p&gt;\r\n&lt;p&gt;It's an imitation
        of &lt;code&gt;black . --check&lt;/code&gt; - the idea is that Symbex can
        now be used to enforce code quality issues like docstrings and the presence
        of type annotations.&lt;/p&gt;\r\n&lt;p&gt;The other new feature is &lt;code&gt;--rexec&lt;/code&gt;.
        This is an extension of the existing &lt;code&gt;--replace&lt;/code&gt; feature,
        which lets you find a symbol in your code and replace its body with new code.&lt;/p&gt;\r\n&lt;p&gt;&lt;code&gt;--rexec&lt;/code&gt;
        takes a shell expression. The body of the matching symbol will be piped into
        that command, and its output will be used as the replacement.&lt;/p&gt;\r\n&lt;p&gt;Which
        means you can do things like this:&lt;/p&gt;\r\n&lt;div class=\"highlight
        highlight-source-shell\"&gt;&lt;pre&gt;symbex my_function \\\r\n  --rexec
        &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;llm
        --system 'add type hints and a docstring'&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;This
        will find &lt;code&gt;def my_function()&lt;/code&gt; and its body, pass that
        through &lt;code&gt;llm&lt;/code&gt; (using the &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;
        default model, but you can specify &lt;code&gt;-m gpt-4&lt;/code&gt; or any
        other model to use something else), and then take the output and update the
        file in-place with the new implementation.&lt;/p&gt;\r\n&lt;p&gt;As a demo,
        I ran it against this:&lt;/p&gt;\r\n&lt;pre&gt;&lt;span class=\"pl-k\"&gt;def&lt;/span&gt;
        &lt;span class=\"pl-en\"&gt;my_function&lt;/span&gt;(&lt;span class=\"pl-s1\"&gt;a&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;b&lt;/span&gt;):\r\n    &lt;span class=\"pl-k\"&gt;return&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;a&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;+&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;b&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;+&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;3&lt;/span&gt;&lt;/pre&gt;\r\n&lt;p&gt;And got
        back:&lt;/p&gt;\r\n&lt;pre&gt;&lt;span class=\"pl-k\"&gt;def&lt;/span&gt;
        &lt;span class=\"pl-en\"&gt;my_function&lt;/span&gt;(&lt;span class=\"pl-s1\"&gt;a&lt;/span&gt;:
        &lt;span class=\"pl-s1\"&gt;int&lt;/span&gt;, &lt;span class=\"pl-s1\"&gt;b&lt;/span&gt;:
        &lt;span class=\"pl-s1\"&gt;int&lt;/span&gt;) &lt;span class=\"pl-c1\"&gt;-&amp;gt;&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;int&lt;/span&gt;:\r\n    &lt;span class=\"pl-s\"&gt;\"\"\"&lt;/span&gt;\r\n&lt;span
        class=\"pl-s\"&gt;    Returns the sum of two integers (a and b) plus 3.&lt;/span&gt;\r\n&lt;span
        class=\"pl-s\"&gt;&lt;/span&gt;\r\n&lt;span class=\"pl-s\"&gt;    Parameters:&lt;/span&gt;\r\n&lt;span
        class=\"pl-s\"&gt;    a (int): The first integer.&lt;/span&gt;\r\n&lt;span
        class=\"pl-s\"&gt;    b (int): The second integer.&lt;/span&gt;\r\n&lt;span
        class=\"pl-s\"&gt;&lt;/span&gt;\r\n&lt;span class=\"pl-s\"&gt;    Returns:&lt;/span&gt;\r\n&lt;span
        class=\"pl-s\"&gt;    int: The sum of a and b plus 3.&lt;/span&gt;\r\n&lt;span
        class=\"pl-s\"&gt;    \"\"\"&lt;/span&gt;\r\n    &lt;span class=\"pl-k\"&gt;return&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;a&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;+&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;b&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;+&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;3&lt;/span&gt;&lt;/pre&gt;\r\n&lt;p&gt;Obviously
        this is fraught with danger, and you should only run this against code that
        has already been committed to Git and hence can be easily recovered... but
        it's a really fun trick!&lt;/p&gt;\r\n&lt;h4&gt;ttok --encode --decode&lt;/h4&gt;\r\n&lt;p&gt;&lt;code&gt;ttok&lt;/code&gt;
        is my CLI tool for counting tokens, as used by LLM models such as GPT-4. &lt;a
        href=\"https://github.com/simonw/ttok/releases/tag/0.2\"&gt;ttok 0.2&lt;/a&gt;
        adds a requested feature to help make tokens easier to understand, best illustrated
        by this demo:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;ttok
        Hello world\r\n&lt;span class=\"pl-c\"&gt;&lt;span class=\"pl-c\"&gt;#&lt;/span&gt;
        Outputs 2 - the number of tokens&lt;/span&gt;\r\nttok Hello world --encode\r\n&lt;span
        class=\"pl-c\"&gt;&lt;span class=\"pl-c\"&gt;#&lt;/span&gt; Outputs 9906 1917
        - the encoded tokens&lt;/span&gt;\r\nttok 9906 1917 --decode\r\n&lt;span class=\"pl-c\"&gt;&lt;span
        class=\"pl-c\"&gt;#&lt;/span&gt; Outputs Hello world - decoding the tokens
        back again&lt;/span&gt;\r\nttok Hello world --encode --tokens\r\n&lt;span
        class=\"pl-c\"&gt;&lt;span class=\"pl-c\"&gt;#&lt;/span&gt; Outputs [b'Hello',
        b' world']&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Being able to
        easily see the encoded tokens including whitespace (the &lt;code&gt;b' world'&lt;/code&gt;
        part) is particularly useful for understanding how the tokens all fit together.&lt;/p&gt;\r\n&lt;p&gt;I
        wrote more about GPT tokenization in &lt;a href=\"https://simonwillison.net/2023/Jun/8/gpt-tokenizers/\"&gt;understanding
        GPT tokenizers&lt;/a&gt;.&lt;/p&gt;\r\n&lt;h4&gt;TIL this week&lt;/h4&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/python/tree-sitter\"&gt;Using tree-sitter
        with Python&lt;/a&gt; - 2023-07-14&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a href=\"https://til.simonwillison.net/yaml/yamlfmt\"&gt;Auto-formatting
        YAML files with yamlfmt&lt;/a&gt; - 2023-07-13&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/python/quick-testing-pyenv\"&gt;Quickly
        testing code in a different Python version using pyenv&lt;/a&gt; - 2023-07-10&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/git/git-filter-repo\"&gt;Using git-filter-repo
        to set commit dates to author dates&lt;/a&gt; - 2023-07-10&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/gpt3/openai-python-functions-data-extraction\"&gt;Using
        OpenAI functions and their Python library for data extraction&lt;/a&gt; -
        2023-07-10&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a href=\"https://til.simonwillison.net/python/pyproject\"&gt;Python
        packages with pyproject.toml and nothing else&lt;/a&gt; - 2023-07-08&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/datasette/syntax-highlighted-code-examples\"&gt;Syntax
        highlighted code examples in Datasette&lt;/a&gt; - 2023-07-02&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/jinja/custom-jinja-tags-with-attributes\"&gt;Custom
        Jinja template tags with attributes&lt;/a&gt; - 2023-07-02&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/macos/wildcard-dns-dnsmasq\"&gt;Local
        wildcard DNS on macOS with dnsmasq&lt;/a&gt; - 2023-06-30&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/discord/discord-github-issues-bot\"&gt;A
        Discord bot to expand issue links to a private GitHub repository&lt;/a&gt;
        - 2023-06-30&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a href=\"https://til.simonwillison.net/github/bulk-edit-github-projects\"&gt;Bulk
        editing status in GitHub Projects&lt;/a&gt; - 2023-06-29&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/python/stdlib-cli-tools\"&gt;CLI tools
        hidden in the Python standard library&lt;/a&gt; - 2023-06-29&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;h4&gt;Releases
        this week&lt;/h4&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a href=\"https://github.com/simonw/symbex/releases/tag/1.1\"&gt;symbex
        1.1&lt;/a&gt;&lt;/strong&gt; - 2023-07-16&lt;br /&gt;Find the Python code
        for specified symbols&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a href=\"https://github.com/simonw/llm-mpt30b/releases/tag/0.1\"&gt;llm-mpt30b
        0.1&lt;/a&gt;&lt;/strong&gt; - 2023-07-12&lt;br /&gt;LLM plugin adding support
        for the MPT-30B language model&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a
        href=\"https://github.com/simonw/llm-markov/releases/tag/0.1\"&gt;llm-markov
        0.1&lt;/a&gt;&lt;/strong&gt; - 2023-07-12&lt;br /&gt;Plugin for LLM adding
        a Markov chain generating model&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a
        href=\"https://github.com/simonw/llm-gpt4all/releases/tag/0.1\"&gt;llm-gpt4all
        0.1&lt;/a&gt;&lt;/strong&gt; - 2023-07-12&lt;br /&gt;Plugin for LLM adding
        support for the GPT4All collection of models&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a
        href=\"https://github.com/simonw/llm-palm/releases/tag/0.1\"&gt;llm-palm 0.1&lt;/a&gt;&lt;/strong&gt;
        - 2023-07-12&lt;br /&gt;Plugin for LLM adding support for Google's PaLM 2
        model&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a href=\"https://github.com/simonw/llm/releases/tag/0.5\"&gt;llm
        0.5&lt;/a&gt;&lt;/strong&gt; - 2023-07-12&lt;br /&gt;Access large language
        models from the command-line&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a
        href=\"https://github.com/simonw/ttok/releases/tag/0.2\"&gt;ttok 0.2&lt;/a&gt;&lt;/strong&gt;
        - 2023-07-10&lt;br /&gt;Count and truncate text based on tokens&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a
        href=\"https://github.com/simonw/strip-tags/releases/tag/0.5.1\"&gt;strip-tags
        0.5.1&lt;/a&gt;&lt;/strong&gt; - 2023-07-09&lt;br /&gt;CLI tool for stripping
        tags from HTML&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a href=\"https://github.com/dogsheep/pocket-to-sqlite/releases/tag/0.2.3\"&gt;pocket-to-sqlite
        0.2.3&lt;/a&gt;&lt;/strong&gt; - 2023-07-09&lt;br /&gt;Create a SQLite database
        containing data from your Pocket account&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a
        href=\"https://github.com/simonw/datasette-render-markdown/releases/tag/2.2\"&gt;datasette-render-markdown
        2.2&lt;/a&gt;&lt;/strong&gt; - 2023-07-02&lt;br /&gt;Datasette plugin for
        rendering Markdown&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a href=\"https://github.com/simonw/asgi-proxy-lib/releases/tag/0.1a0\"&gt;asgi-proxy-lib
        0.1a0&lt;/a&gt;&lt;/strong&gt; - 2023-07-01&lt;br /&gt;An ASGI function for
        proxying to a backend over HTTP&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a
        href=\"https://github.com/simonw/datasette-upload-csvs/releases/tag/0.8.3\"&gt;datasette-upload-csvs
        0.8.3&lt;/a&gt;&lt;/strong&gt; - 2023-06-28&lt;br /&gt;Datasette plugin for
        uploading CSV files and converting them to database tables&lt;/li&gt;\r\n&lt;/ul&gt;\n\n</summary><category
        term=\"symbex\"/><category term=\"sqliteutils\"/><category term=\"llm\"/><category
        term=\"plugins\"/><category term=\"tutorials\"/><category term=\"ai\"/><category
        term=\"homebrewllms\"/><category term=\"llms\"/><category term=\"datasette\"/><category
        term=\"generativeai\"/><category term=\"projects\"/><category term=\"weeknotes\"/></entry><entry><title>Quoting
        Ethan Mollick</title><link href=\"http://simonwillison.net/2023/Jul/16/documentation-by-rumor/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-16T00:12:10+00:00</published><updated>2023-07-16T00:12:10+00:00</updated><id>http://simonwillison.net/2023/Jul/16/documentation-by-rumor/#atom-everything</id><summary
        type=\"html\">\n    &lt;blockquote cite=\"https://www.oneusefulthing.org/p/how-to-use-ai-to-do-stuff-an-opinionated\"&gt;&lt;p&gt;Increasingly
        powerful AI systems are being released at an increasingly rapid pace. [...]
        And yet not a single AI lab seems to have provided any user documentation.
        Instead, the only user guides out there appear to be Twitter influencer threads.
        Documentation-by-rumor is a weird choice for organizations claiming to be
        concerned about proper use of their technologies, but here we are.&lt;/p&gt;&lt;/blockquote&gt;&lt;p
        class=\"cite\"&gt;&amp;mdash; &lt;a href=\"https://www.oneusefulthing.org/p/how-to-use-ai-to-do-stuff-an-opinionated\"&gt;Ethan
        Mollick&lt;/a&gt;\n\n</summary><category term=\"ethanmollick\"/><category
        term=\"ai\"/><category term=\"generativeai\"/><category term=\"ethics\"/></entry><entry><title>Quoting
        Kevin Roose</title><link href=\"http://simonwillison.net/2023/Jul/13/kevin-roose/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-13T22:23:05+00:00</published><updated>2023-07-13T22:23:05+00:00</updated><id>http://simonwillison.net/2023/Jul/13/kevin-roose/#atom-everything</id><summary
        type=\"html\">\n    &lt;blockquote cite=\"https://www.nytimes.com/2023/07/11/technology/anthropic-ai-claude-chatbot.html\"&gt;&lt;p&gt;Not
        every conversation I had at Anthropic revolved around existential risk. But
        dread was a dominant theme. At times, I felt like a food writer who was assigned
        to cover a trendy new restaurant, only to discover that the kitchen staff
        wanted to talk about nothing but food poisoning.&lt;/p&gt;&lt;/blockquote&gt;&lt;p
        class=\"cite\"&gt;&amp;mdash; &lt;a href=\"https://www.nytimes.com/2023/07/11/technology/anthropic-ai-claude-chatbot.html\"&gt;Kevin
        Roose&lt;/a&gt;\n\n</summary><category term=\"anthropic\"/><category term=\"ai\"/><category
        term=\"ethics\"/><category term=\"generativeai\"/></entry><entry><title>What
        AI can do with a toolbox... Getting started with Code Interpreter</title><link
        href=\"http://simonwillison.net/2023/Jul/12/what-ai-can-do-with-a-toolbox-getting-started-with-code-interpre/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-12T20:57:34+00:00</published><updated>2023-07-12T20:57:34+00:00</updated><id>http://simonwillison.net/2023/Jul/12/what-ai-can-do-with-a-toolbox-getting-started-with-code-interpre/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://www.oneusefulthing.org/p/what-ai-can-do-with-a-toolbox-getting\"&gt;What
        AI can do with a toolbox... Getting started with Code Interpreter&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Ethan
        Mollick has been doing some very creative explorations of ChatGPT Code Interpreter
        over the past few months, and has tied a lot of them together into this useful
        introductory tutorial.&lt;/p&gt;\n\n\n\n</summary><category term=\"ethanmollick\"/><category
        term=\"generativeai\"/><category term=\"openai\"/><category term=\"chatgpt\"/><category
        term=\"ai\"/><category term=\"llms\"/></entry><entry><title>claude.ai</title><link
        href=\"http://simonwillison.net/2023/Jul/12/claudeai/#atom-everything\" rel=\"alternate\"/><published>2023-07-12T16:39:38+00:00</published><updated>2023-07-12T16:39:38+00:00</updated><id>http://simonwillison.net/2023/Jul/12/claudeai/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://claude.ai/\"&gt;claude.ai&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Anthropic&amp;#x27;s
        new Claude 2 model is available to use online, and it has a 100k token context
        window and the ability to upload files to it - I tried uploading a text file
        with 34,000 tokens in it (according to my ttok CLI tool, counting using the
        GPT-3.5 tokenizer) and it gave me a workable summary.&lt;/p&gt;\n\n\n\n</summary><category
        term=\"llms\"/><category term=\"ai\"/><category term=\"generativeai\"/><category
        term=\"anthropic\"/></entry><entry><title>My LLM CLI tool now supports self-hosted
        language models via plugins</title><link href=\"http://simonwillison.net/2023/Jul/12/llm/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-12T14:24:40+00:00</published><updated>2023-07-12T14:24:40+00:00</updated><id>http://simonwillison.net/2023/Jul/12/llm/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://llm.datasette.io/\"&gt;LLM&lt;/a&gt;
        is my command-line utility and Python library for working with large language
        models such as GPT-4. I just released version 0.5 with a huge new feature:
        you can now install plugins that add support for &lt;strong&gt;additional
        models&lt;/strong&gt; to the tool, including models that can run on your own
        hardware.&lt;/p&gt;\r\n&lt;p&gt;Highlights of today's release:&lt;/p&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;Plugins
        to add support for 17 openly licensed models from the &lt;a href=\"https://gpt4all.io/\"&gt;GPT4All
        project&lt;/a&gt; that can run directly on your device, plus Mosaic's MPT-30B
        self-hosted model and Google's PaLM 2 (via their API).&lt;/li&gt;\r\n&lt;li&gt;This
        means you can &lt;code&gt;pip install&lt;/code&gt; (or &lt;code&gt;brew install&lt;/code&gt;)
        models along with a CLI tool for using them!&lt;/li&gt;\r\n&lt;li&gt;A &lt;a
        href=\"https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html\"&gt;detailed
        tutorial&lt;/a&gt; describing how to build new plugins that add support for
        additional models.&lt;/li&gt;\r\n&lt;li&gt;A documented &lt;a href=\"https://llm.datasette.io/en/stable/python-api.html\"&gt;Python
        API&lt;/a&gt; for running prompts through any model provided by a plugin,
        plus a way of continuing a conversation across multiple prompts.&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;h4&gt;How
        to try it out&lt;/h4&gt;\r\n&lt;p&gt;First, install LLM. You can install it
        with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;\r\n&lt;div class=\"highlight
        highlight-source-shell\"&gt;&lt;pre&gt;pip install llm&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Or
        &lt;a href=\"https://pypa.github.io/pipx/\"&gt;pipx&lt;/a&gt;:&lt;/p&gt;\r\n&lt;div
        class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;pipx install llm&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Or
        if you don't yet have a Python 3 environment, you can use &lt;a href=\"https://brew.sh/\"&gt;Homebrew&lt;/a&gt;
        (much slower):&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;brew
        install simonw/llm/llm&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;The default tool
        can work with OpenAI's models via their API, provided you have an API key.
        You can see &lt;a href=\"https://llm.datasette.io/en/latest/usage.html\"&gt;usage
        instructions for that here&lt;/a&gt;.&lt;/p&gt;\r\n&lt;p&gt;But let's do something
        more interesting than that: Let's install a model that can run on our own
        machine!&lt;/p&gt;\r\n&lt;p&gt;We'll use the new &lt;a href=\"https://github.com/simonw/llm-gpt4all\"&gt;llm-gpt4all
        plugin&lt;/a&gt;, which installs models published by the &lt;a href=\"https://gpt4all.io/\"&gt;GPT4All
        project&lt;/a&gt; by Nomic AI.&lt;/p&gt;\r\n&lt;p&gt;Install the plugin like
        this:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;llm
        install llm-gpt4all&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Now let's run a prompt
        against a small model. LLM will download the model file the first time you
        query that model.&lt;/p&gt;\r\n&lt;p&gt;We'll start with &lt;code&gt;ggml-vicuna-7b-1&lt;/code&gt;,
        a 4.21GB download which should run if you have at least 8GB of RAM.&lt;/p&gt;\r\n&lt;p&gt;To
        run the prompt, try this:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;llm
        -m ggml-vicuna-7b-1 &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;The
        capital of France?&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;You'll
        see a progress bar showing the download of the model, followed by the answer
        to the prompt, generated a word at a time.&lt;/p&gt;\r\n&lt;p&gt;&lt;img src=\"https://static.simonwillison.net/static/2023/llm-vicuna.gif\"
        alt=\"Animated screenshot. Running that command produces a progress bar as
        the 4.21GB model downloads - once the download finishes it spits out the sentence
        Paris is the capital of France one word at a time. Then the user types llm
        logs -n 1 and sees a JSON log revealing the details about the prompt that
        were saved in the database.\" style=\"max-width: 100%;\" /&gt;&lt;/p&gt;\r\n&lt;p&gt;All
        prompts and responses are automatically logged to a SQLite database. Calling
        &lt;code&gt;llm logs&lt;/code&gt; with a &lt;code&gt;-n 1&lt;/code&gt; argument
        will show the most recent record:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;llm
        logs -n 1&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;This outputs something like
        the following:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-json\"&gt;&lt;pre&gt;[\r\n
        \ {\r\n    &lt;span class=\"pl-ent\"&gt;\"id\"&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;01h549p8r12ac1980crbr9yhjf&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;,\r\n
        \   &lt;span class=\"pl-ent\"&gt;\"model\"&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;ggml-vicuna-7b-1&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;,\r\n
        \   &lt;span class=\"pl-ent\"&gt;\"prompt\"&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;The capital of France?&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;,\r\n
        \   &lt;span class=\"pl-ent\"&gt;\"system\"&lt;/span&gt;: &lt;span class=\"pl-c1\"&gt;null&lt;/span&gt;,\r\n
        \   &lt;span class=\"pl-ent\"&gt;\"prompt_json\"&lt;/span&gt;: &lt;span class=\"pl-c1\"&gt;null&lt;/span&gt;,\r\n
        \   &lt;span class=\"pl-ent\"&gt;\"options_json\"&lt;/span&gt;: {},\r\n    &lt;span
        class=\"pl-ent\"&gt;\"response\"&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;Paris is the capital of France.&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;,\r\n    &lt;span class=\"pl-ent\"&gt;\"response_json\"&lt;/span&gt;:
        {\r\n      &lt;span class=\"pl-ent\"&gt;\"full_prompt\"&lt;/span&gt;: &lt;span
        class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;### Human: &lt;span
        class=\"pl-cce\"&gt;\\n&lt;/span&gt;The capital of France?&lt;span class=\"pl-cce\"&gt;\\n&lt;/span&gt;###
        Assistant:&lt;span class=\"pl-cce\"&gt;\\n&lt;/span&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;\r\n
        \   },\r\n    &lt;span class=\"pl-ent\"&gt;\"conversation_id\"&lt;/span&gt;:
        &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;01h549p8r0abz6ebwd7agmjmgy&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;,\r\n    &lt;span class=\"pl-ent\"&gt;\"duration_ms\"&lt;/span&gt;:
        &lt;span class=\"pl-c1\"&gt;9511&lt;/span&gt;,\r\n    &lt;span class=\"pl-ent\"&gt;\"datetime_utc\"&lt;/span&gt;:
        &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2023-07-12T05:37:44.407233&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;,\r\n    &lt;span class=\"pl-ent\"&gt;\"conversation_name\"&lt;/span&gt;:
        &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;The
        capital of France?&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;,\r\n
        \   &lt;span class=\"pl-ent\"&gt;\"conversation_model\"&lt;/span&gt;: &lt;span
        class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;ggml-vicuna-7b-1&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;\r\n  }\r\n]&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;You
        can see a full list of available models by running the &lt;code&gt;llm models
        list&lt;/code&gt; command. The &lt;code&gt;llm-gpt4all&lt;/code&gt; plugin
        adds 17 models to the tool:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;llm
        models list&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;I've installed all three plugins
        that provide extra models, so I see the following:&lt;/p&gt;\r\n&lt;pre&gt;&lt;code&gt;OpenAI
        Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\r\nOpenAI Chat: gpt-3.5-turbo-16k
        (aliases: chatgpt-16k, 3.5-16k)\r\nOpenAI Chat: gpt-4 (aliases: 4, gpt4)\r\nOpenAI
        Chat: gpt-4-32k (aliases: 4-32k)\r\nPaLM 2: chat-bison-001 (aliases: palm,
        palm2)\r\ngpt4all: orca-mini-3b - Orca (Small), 1.80GB download, needs 4GB
        RAM (installed)\r\ngpt4all: ggml-gpt4all-j-v1 - Groovy, 3.53GB download, needs
        8GB RAM (installed)\r\ngpt4all: orca-mini-7b - Orca, 3.53GB download, needs
        8GB RAM (installed)\r\ngpt4all: ggml-vicuna-7b-1 - Vicuna, 3.92GB download,
        needs 8GB RAM (installed)\r\ngpt4all: ggml-mpt-7b-chat - MPT Chat, 4.52GB
        download, needs 8GB RAM (installed)\r\ngpt4all: ggml-replit-code-v1-3b - Replit,
        4.84GB download, needs 4GB RAM (installed)\r\ngpt4all: ggml-vicuna-13b-1 -
        Vicuna (large), 7.58GB download, needs 16GB RAM (installed)\r\ngpt4all: nous-hermes-13b
        - Hermes, 7.58GB download, needs 16GB RAM (installed)\r\ngpt4all: ggml-model-gpt4all-falcon-q4_0
        - GPT4All Falcon, 3.78GB download, needs 8GB RAM\r\ngpt4all: ggml-wizardLM-7B
        - Wizard, 3.92GB download, needs 8GB RAM\r\ngpt4all: ggml-mpt-7b-base - MPT
        Base, 4.52GB download, needs 8GB RAM\r\ngpt4all: ggml-mpt-7b-instruct - MPT
        Instruct, 4.52GB download, needs 8GB RAM\r\ngpt4all: orca-mini-13b - Orca
        (Large), 6.82GB download, needs 16GB RAM\r\ngpt4all: GPT4All-13B-snoozy -
        Snoozy, 7.58GB download, needs 16GB RAM\r\ngpt4all: ggml-nous-gpt4-vicuna-13b
        - Nous Vicuna, 7.58GB download, needs 16GB RAM\r\ngpt4all: ggml-stable-vicuna-13B
        - Stable Vicuna, 7.58GB download, needs 16GB RAM\r\ngpt4all: wizardLM-13B-Uncensored
        - Wizard Uncensored, 7.58GB download, needs 16GB RAM\r\nMpt30b: mpt30b (aliases:
        mpt)\r\n&lt;/code&gt;&lt;/pre&gt;\r\n&lt;p&gt;In addition to the &lt;code&gt;gpt4all&lt;/code&gt;
        models I can also run &lt;code&gt;PaLM 2&lt;/code&gt; from Google and &lt;code&gt;mpt30b&lt;/code&gt;
        from Mosaic, as well as the four OpenAI models.&lt;/p&gt;\r\n&lt;p&gt;Models
        have aliases, so in some cases you can run &lt;code&gt;llm -m mpt&lt;/code&gt;
        instead of &lt;code&gt;llm -m mpt30b&lt;/code&gt;.&lt;/p&gt;\r\n&lt;p&gt;We'll
        try one more model. Google's PaLM 2 was released &lt;a href=\"https://blog.google/technology/ai/google-palm-2-ai-large-language-model/\"&gt;a
        few weeks ago&lt;/a&gt;, and can be accessed through their &lt;a href=\"https://developers.generativeai.google/\"&gt;PaLM
        API&lt;/a&gt;.&lt;/p&gt;\r\n&lt;p&gt;&lt;a href=\"https://makersuite.google.com/waitlist\"&gt;Obtain
        an API key&lt;/a&gt; for that, and install the &lt;code&gt;llm-palm&lt;/code&gt;
        plugin:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;pip
        install llm-palm&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Set your API key like
        this:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;llm
        keys &lt;span class=\"pl-c1\"&gt;set&lt;/span&gt; palm&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Now
        you can run prompts against it like this:&lt;/p&gt;\r\n&lt;div class=\"highlight
        highlight-source-shell\"&gt;&lt;pre&gt;llm -m palm &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;Ten absurd names for a pet giraffe&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;PaLM
        replies:&lt;/p&gt;\r\n&lt;blockquote&gt;\r\n&lt;p&gt;Here are ten absurd names
        for a pet giraffe:&lt;/p&gt;\r\n&lt;ol&gt;\r\n&lt;li&gt;Stretch&lt;/li&gt;\r\n&lt;li&gt;Necky&lt;/li&gt;\r\n&lt;li&gt;Long
        Legs&lt;/li&gt;\r\n&lt;li&gt;Tree Top&lt;/li&gt;\r\n&lt;li&gt;Tall Boy&lt;/li&gt;\r\n&lt;li&gt;High
        Five&lt;/li&gt;\r\n&lt;li&gt;Sky Walker&lt;/li&gt;\r\n&lt;li&gt;Cloud Chaser&lt;/li&gt;\r\n&lt;li&gt;Star
        Gazer&lt;/li&gt;\r\n&lt;li&gt;Horizon Hopper&lt;/li&gt;\r\n&lt;/ol&gt;\r\n&lt;p&gt;I
        hope you find these names amusing!&lt;/p&gt;\r\n&lt;/blockquote&gt;\r\n&lt;p&gt;This
        also gets logged to the database - run &lt;code&gt;llm logs -n 1&lt;/code&gt;
        again to see the log entry.&lt;/p&gt;\r\n&lt;p&gt;LLM supports continuing
        a conversation with more prompts. We can run another prompt through PaLM as
        part of the same conversation like this:&lt;/p&gt;\r\n&lt;div class=\"highlight
        highlight-source-shell\"&gt;&lt;pre&gt;llm --continue &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;\"&lt;/span&gt;3 more and make them weirder&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;PaLM
        replies:&lt;/p&gt;\r\n&lt;blockquote&gt;\r\n&lt;p&gt;Sure, here are three
        more absurd names for a pet giraffe, even weirder than the first ten:&lt;/p&gt;\r\n&lt;ol&gt;\r\n&lt;li&gt;Giraffey
        McFierceface&lt;/li&gt;\r\n&lt;li&gt;Longneck von Longneck&lt;/li&gt;\r\n&lt;li&gt;The
        Giraffe Whisperer&lt;/li&gt;\r\n&lt;/ol&gt;\r\n&lt;p&gt;I hope you find these
        names even more amusing than the first ten!&lt;/p&gt;\r\n&lt;/blockquote&gt;\r\n&lt;p&gt;Using
        &lt;code&gt;-c/--continue&lt;/code&gt; will continue the most previous conversation.
        You can also pass a conversation ID (available in the output from &lt;code&gt;llm
        logs&lt;/code&gt;) using &lt;code&gt;--cid ID&lt;/code&gt; to reply to an
        older conversation thread.&lt;/p&gt;\r\n&lt;h4&gt;Adding a new model&lt;/h4&gt;\r\n&lt;p&gt;I've
        tried to make it as easy as possible to add support for additional models
        through writing plugins. The tutorial &lt;a href=\"https://llm.datasette.io/en/latest/plugins/tutorial-model-plugin.html\"&gt;Writing
        a plugin to support a new model&lt;/a&gt; is extremely thorough, and includes
        detailed descriptions of how to start a new plugin, set up a development environment
        for it, integrate it with a new model and then package it for distribution.&lt;/p&gt;\r\n&lt;p&gt;The
        tutorial uses a &lt;a href=\"https://en.wikipedia.org/wiki/Markov_chain\"&gt;Markov
        chain&lt;/a&gt; implementation as an example, possibly the simplest possible
        form of language model.&lt;/p&gt;\r\n&lt;p&gt;The source code of the other
        existing plugins should help show how to integrate with more complex models:&lt;/p&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://github.com/simonw/llm-palm/blob/main/llm_palm/__init__.py\"&gt;llm_palm/__init__.py&lt;/a&gt;
        demonstrates how to integrate with a model exposed via Google's API library.&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://github.com/simonw/llm/blob/main/llm/default_plugins/openai_models.py\"&gt;llm/default_plugins/openai_models.py&lt;/a&gt;
        demonstrates integration against the OpenAI APIs.&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://github.com/simonw/llm-gpt4all/blob/main/llm_gpt4all.py\"&gt;llm_gpt4all.py&lt;/a&gt;
        shows an integration with the &lt;a href=\"https://docs.gpt4all.io/gpt4all_python.html\"&gt;gpt4all&lt;/a&gt;
        Python library.&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a href=\"https://github.com/simonw/llm-mpt30b/blob/main/llm_mpt30b.py\"&gt;llm_mpt30b.py&lt;/a&gt;
        demonstrates a direct integration against a model using the &lt;a href=\"https://github.com/marella/ctransformers\"&gt;ctransformers&lt;/a&gt;
        library.&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;h4&gt;Using LLM from Python&lt;/h4&gt;\r\n&lt;p&gt;LLM
        was originally designed to be used from the command-line, but in version 0.5
        I've expanded it to work as a Python library as well.&lt;/p&gt;\r\n&lt;p&gt;The
        &lt;a href=\"https://llm.datasette.io/en/stable/python-api.html\"&gt;documentation
        for that is here&lt;/a&gt;, but here's the short version:&lt;/p&gt;\r\n&lt;pre&gt;&lt;span
        class=\"pl-k\"&gt;import&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;llm&lt;/span&gt;\r\n\r\n&lt;span
        class=\"pl-s1\"&gt;model&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;llm&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;get_model&lt;/span&gt;(&lt;span
        class=\"pl-s\"&gt;\"gpt-3.5-turbo\"&lt;/span&gt;)\r\n&lt;span class=\"pl-s1\"&gt;model&lt;/span&gt;.&lt;span
        class=\"pl-s1\"&gt;key&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s\"&gt;'YOUR_API_KEY_HERE'&lt;/span&gt;\r\n\r\n&lt;span
        class=\"pl-s1\"&gt;response&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;model&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;prompt&lt;/span&gt;(&lt;span
        class=\"pl-s\"&gt;\"Five surprising names for a pet pelican\"&lt;/span&gt;)\r\n&lt;span
        class=\"pl-k\"&gt;for&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;chunk&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;in&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;response&lt;/span&gt;:\r\n
        \   &lt;span class=\"pl-en\"&gt;print&lt;/span&gt;(&lt;span class=\"pl-s1\"&gt;chunk&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;end&lt;/span&gt;&lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;&lt;span
        class=\"pl-s\"&gt;\"\"&lt;/span&gt;)\r\n\r\n&lt;span class=\"pl-c\"&gt;# Or
        wait for the whole response to be ready:&lt;/span&gt;\r\n&lt;span class=\"pl-en\"&gt;print&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;response&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;text&lt;/span&gt;())&lt;/pre&gt;\r\n&lt;p&gt;Any
        model that can be installed via a plugin can be accessed in the same way.&lt;/p&gt;\r\n&lt;p&gt;The
        API also supports conversations, where multiple prompts are sent to the model
        as part of the same persistent context:&lt;/p&gt;\r\n&lt;pre&gt;&lt;span class=\"pl-k\"&gt;import&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;llm&lt;/span&gt;\r\n\r\n&lt;span class=\"pl-s1\"&gt;model&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;llm&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;get_model&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;\"ggml-mpt-7b-chat\"&lt;/span&gt;)\r\n\r\n&lt;span
        class=\"pl-s1\"&gt;conversation&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;model&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;conversation&lt;/span&gt;()\r\n&lt;span
        class=\"pl-s1\"&gt;r1&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;conversation&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;prompt&lt;/span&gt;(&lt;span
        class=\"pl-s\"&gt;\"Capital of Spain?\"&lt;/span&gt;)\r\n&lt;span class=\"pl-en\"&gt;print&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;r1&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;text&lt;/span&gt;())\r\n\r\n&lt;span
        class=\"pl-s1\"&gt;r2&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;conversation&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;prompt&lt;/span&gt;(&lt;span
        class=\"pl-s\"&gt;\"What language do they speak there?\"&lt;/span&gt;)\r\n&lt;span
        class=\"pl-en\"&gt;print&lt;/span&gt;(&lt;span class=\"pl-s1\"&gt;r2&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;text&lt;/span&gt;())&lt;/pre&gt;\r\n&lt;h4&gt;What's next?&lt;/h4&gt;\r\n&lt;p&gt;You
        can follow ongoing LLM development in the &lt;a href=\"https://github.com/simonw/llm/issues\"&gt;GitHub
        repository issues&lt;/a&gt;.&lt;/p&gt;\r\n&lt;p&gt;My next priority is to
        get &lt;a href=\"https://openai.com/blog/function-calling-and-other-api-updates\"&gt;OpenAI
        functions&lt;/a&gt; working. I want to provide the option for other models
        from plugins to implement a similar pattern using &lt;a href=\"https://til.simonwillison.net/llms/python-react-pattern\"&gt;the
        reAct pattern&lt;/a&gt; as well.&lt;/p&gt;\r\n&lt;p&gt;I'll likely do this
        by implementing the concept of a \"chain\" of LLM calls, where a single prompt
        might lead to multiple calls being made to the LLM based on logic that decides
        if another call is necessary.&lt;/p&gt;\r\n&lt;p&gt;I'm also planning a web
        interface. I'm particularly excited about the potential for plugins here -
        I love the idea of plugins that provide new interfaces for interacting with
        language models that go beyond the chat interfaces we've mostly seen so far.&lt;/p&gt;\n\n</summary><category
        term=\"generativeai\"/><category term=\"projects\"/><category term=\"ai\"/><category
        term=\"homebrewllms\"/><category term=\"llms\"/><category term=\"llm\"/></entry><entry><title>Latent
        Space: Code Interpreter == GPT 4.5</title><link href=\"http://simonwillison.net/2023/Jul/10/latent-space-code-interpreter-gpt-45/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-10T22:06:19+00:00</published><updated>2023-07-10T22:06:19+00:00</updated><id>http://simonwillison.net/2023/Jul/10/latent-space-code-interpreter-gpt-45/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://www.latent.space/p/code-interpreter\"&gt;Latent
        Space: Code Interpreter == GPT 4.5&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;I presented
        as part of this Latent Space episode over the weekend, talking about the newly
        released ChatGPT Code Interpreter mode with swyx, Alex Volkov, Daniel Wilson
        and more. swyx did a great job editing our Twitter Spaces conversation into
        a podcast and writing up a detailed executive summary, posted here along with
        the transcript. If you&amp;#x27;re curious you can listen to the first 15
        minutes to get a great high-level explanation of Code Interpreter, or stick
        around for the full two hours for all of the details.&lt;br&gt;&lt;br&gt;Apparently
        our live conversation had 17,000+ listeners!&lt;/p&gt;\n\n    &lt;p&gt;Via
        &lt;a href=\"https://twitter.com/swyx/status/1678512823457165312\"&gt;@swyx&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"swyx\"/><category term=\"generativeai\"/><category term=\"chatgpt\"/><category
        term=\"ai\"/><category term=\"llms\"/><category term=\"speaking\"/><category
        term=\"podcasts\"/></entry><entry><title>Lima VM - Linux Virtual Machines
        On macOS</title><link href=\"http://simonwillison.net/2023/Jul/10/lima/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-10T19:01:21+00:00</published><updated>2023-07-10T19:01:21+00:00</updated><id>http://simonwillison.net/2023/Jul/10/lima/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://earthly.dev/blog/lima/\"&gt;Lima
        VM - Linux Virtual Machines On macOS&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;This looks
        really useful: &amp;quot;brew install lima&amp;quot; to install, then &amp;quot;limactl
        start default&amp;quot; to start an Ubuntu VM running and &amp;quot;lima&amp;quot;
        to get a shell. Julia Evans wrote about the tool this morning, and here Adam
        Gordon Bell includes details on adding a writable directory (by default lima
        mounts your macOS home directory in read-only mode).&lt;/p&gt;\n\n    &lt;p&gt;Via
        &lt;a href=\"https://jvns.ca/blog/2023/07/10/lima--a-nice-way-to-run-linux-vms-on-mac/\"&gt;Lima:
        a nice way to run Linux VMs on Mac&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"virtualization\"/><category term=\"macosx\"/><category term=\"linux\"/><category
        term=\"juliaevans\"/></entry><entry><title>Quoting jbreckmckye</title><link
        href=\"http://simonwillison.net/2023/Jul/10/jbreckmckye/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-10T18:53:41+00:00</published><updated>2023-07-10T18:53:41+00:00</updated><id>http://simonwillison.net/2023/Jul/10/jbreckmckye/#atom-everything</id><summary
        type=\"html\">\n    &lt;blockquote cite=\"https://news.ycombinator.com/item?id=36667469#36669622\"&gt;&lt;p&gt;At
        The Guardian we had a pretty direct way to fix this [the problem of zombie
        feature flags]: experiments were associated with expiry dates, and if your
        team&amp;#x27;s experiments expired the build system simply wouldn&amp;#x27;t
        process your jobs without outside intervention. Seems harsh, but I&amp;#x27;ve
        found with many orgs the only way to fix negative externalities in a shared
        codebase is a tool that says &amp;quot;you broke your promises, now we break
        your builds&amp;quot;.&lt;/p&gt;&lt;/blockquote&gt;&lt;p class=\"cite\"&gt;&amp;mdash;
        &lt;a href=\"https://news.ycombinator.com/item?id=36667469#36669622\"&gt;jbreckmckye&lt;/a&gt;\n\n</summary><category
        term=\"featureflags\"/><category term=\"continuousintegration\"/></entry><entry><title>Why
        We Replaced Firecracker with QEMU</title><link href=\"http://simonwillison.net/2023/Jul/10/why-we-replaced-firecracker-with-qemu/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-10T15:09:03+00:00</published><updated>2023-07-10T15:09:03+00:00</updated><id>http://simonwillison.net/2023/Jul/10/why-we-replaced-firecracker-with-qemu/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://hocus.dev/blog/qemu-vs-firecracker/\"&gt;Why
        We Replaced Firecracker with QEMU&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Hocus are
        building a self-hosted alternative to cloud development environment tools
        like GitPod and Codespaces. They moved away from Firecracker because it&amp;#x27;s
        optimized for short-running (AWS Lambda style) functions - which means it
        never releases allocated RAM or storage volume space back to the host machine
        unless the container is entirely restarted. It also lacks GPU support.&lt;/p&gt;\n\n
        \   &lt;p&gt;Via &lt;a href=\"https://news.ycombinator.com/item?id=36666782\"&gt;Hacker
        News&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category term=\"qemu\"/><category
        term=\"virtualization\"/><category term=\"firecracker\"/></entry><entry><title>Quoting
        Matt Webb</title><link href=\"http://simonwillison.net/2023/Jul/9/matt-webb/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-09T15:29:54+00:00</published><updated>2023-07-09T15:29:54+00:00</updated><id>http://simonwillison.net/2023/Jul/9/matt-webb/#atom-everything</id><summary
        type=\"html\">\n    &lt;blockquote cite=\"https://interconnected.org/home/2023/07/07/whispering\"&gt;&lt;p&gt;It
        feels pretty likely that prompting or chatting with AI agents is going to
        be a major way that we interact with computers into the future, and whereas
        there\u2019s not a huge spread in the ability between people who are not super
        good at tapping on icons on their smartphones and people who are, when it
        comes to working with AI it seems like we\u2019ll have a high dynamic range.
        Prompting opens the door for non-technical virtuosos in a way that we haven\u2019t
        seen with modern computers, outside of maybe Excel.&lt;/p&gt;&lt;/blockquote&gt;&lt;p
        class=\"cite\"&gt;&amp;mdash; &lt;a href=\"https://interconnected.org/home/2023/07/07/whispering\"&gt;Matt
        Webb&lt;/a&gt;\n\n</summary><category term=\"mattwebb\"/><category term=\"promptengineering\"/><category
        term=\"generativeai\"/><category term=\"ai\"/><category term=\"llms\"/></entry><entry><title>Tech
        debt metaphor maximalism</title><link href=\"http://simonwillison.net/2023/Jul/8/tech-debt-metaphor-maximalism/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-08T05:11:24+00:00</published><updated>2023-07-08T05:11:24+00:00</updated><id>http://simonwillison.net/2023/Jul/8/tech-debt-metaphor-maximalism/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://apenwarr.ca/log/20230605\"&gt;Tech
        debt metaphor maximalism&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;I&amp;#x27;ve long
        been a fan of the metaphor of technical debt, because it implies that taking
        on some debt is OK provided you&amp;#x27;re strategic about how much you take
        on and how quickly you pay it off. Avery Pennarun provides the definitive
        guide to thinking about technical debt, including an extremely worthwhile
        explanation of how financial debt works as well.&lt;/p&gt;\n\n    &lt;p&gt;Via
        &lt;a href=\"https://news.ycombinator.com/item?id=36635496\"&gt;Hacker News&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"technicaldebt\"/><category term=\"averypennarun\"/><category term=\"softwareengineering\"/></entry><entry><title>Stamina:
        tutorial</title><link href=\"http://simonwillison.net/2023/Jul/4/stamina-tutorial/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-04T20:13:54+00:00</published><updated>2023-07-04T20:13:54+00:00</updated><id>http://simonwillison.net/2023/Jul/4/stamina-tutorial/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://stamina.hynek.me/en/stable/tutorial.html\"&gt;Stamina:
        tutorial&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Stamina is Hynek&amp;#x27;s new Python
        library that implements an opinionated wrapper on top of Tenacity, providing
        a decorator for easily implementing exponential backoff retires. This tutorial
        includes a concise, clear explanation as to why this is such an important
        concept in building distributed systems.&lt;/p&gt;\n\n    &lt;p&gt;Via &lt;a
        href=\"https://twitter.com/hynek/status/1676210531403718657\"&gt;@hynek&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"softwarearchitecture\"/><category term=\"python\"/><category term=\"hynekschlawack\"/></entry><entry><title>Data
        analysis with SQLite and Python</title><link href=\"http://simonwillison.net/2023/Jul/2/data-analysis-with-sqlite-and-python/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-02T16:48:23+00:00</published><updated>2023-07-02T16:48:23+00:00</updated><id>http://simonwillison.net/2023/Jul/2/data-analysis-with-sqlite-and-python/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://datasette.io/tutorials/data-analysis\"&gt;Data
        analysis with SQLite and Python&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;I turned my
        2hr45m workshop from PyCon into the latest official tutorial on the Datasette
        website. It includes an extensive handout which should be useful independently
        of the video itself.&lt;/p&gt;\n\n\n\n</summary><category term=\"speaking\"/><category
        term=\"sqlite\"/><category term=\"datasette\"/><category term=\"python\"/></entry><entry><title>Quoting
        Paul Graham</title><link href=\"http://simonwillison.net/2023/Jul/1/paul-graham/#atom-everything\"
        rel=\"alternate\"/><published>2023-07-01T16:14:38+00:00</published><updated>2023-07-01T16:14:38+00:00</updated><id>http://simonwillison.net/2023/Jul/1/paul-graham/#atom-everything</id><summary
        type=\"html\">\n    &lt;blockquote cite=\"http://paulgraham.com/greatwork.html\"&gt;&lt;p&gt;Once
        you&amp;#x27;ve found something you&amp;#x27;re excessively interested in,
        the next step is to learn enough about it to get you to one of the frontiers
        of knowledge. Knowledge expands fractally, and from a distance its edges look
        smooth, but once you learn enough to get close to one, they turn out to be
        full of gaps.&lt;/p&gt;&lt;/blockquote&gt;&lt;p class=\"cite\"&gt;&amp;mdash;
        &lt;a href=\"http://paulgraham.com/greatwork.html\"&gt;Paul Graham&lt;/a&gt;\n\n</summary><category
        term=\"paulgraham\"/></entry><entry><title>Databricks Signs Definitive Agreement
        to Acquire MosaicML, a Leading Generative AI Platform</title><link href=\"http://simonwillison.net/2023/Jun/30/databricks-mosaicml/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-30T01:43:56+00:00</published><updated>2023-06-30T01:43:56+00:00</updated><id>http://simonwillison.net/2023/Jun/30/databricks-mosaicml/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://www.databricks.com/company/newsroom/press-releases/databricks-signs-definitive-agreement-acquire-mosaicml-leading-generative-ai-platform\"&gt;Databricks
        Signs Definitive Agreement to Acquire MosaicML, a Leading Generative AI Platform&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;MosaicML
        are the team behind MPT-7B and MPT-30B, two of the most impressive openly
        licensed LLMs. They just got acquired by Databricks for $1.3 billion dollars.&lt;/p&gt;\n\n\n\n</summary><category
        term=\"opensource\"/><category term=\"generativeai\"/><category term=\"ai\"/><category
        term=\"homebrewllms\"/><category term=\"llms\"/></entry><entry><title>abacaj/mpt-30B-inference</title><link
        href=\"http://simonwillison.net/2023/Jun/29/mpt-30b-inference/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-29T03:27:47+00:00</published><updated>2023-06-29T03:27:47+00:00</updated><id>http://simonwillison.net/2023/Jun/29/mpt-30b-inference/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://github.com/abacaj/mpt-30B-inference\"&gt;abacaj/mpt-30B-inference&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;MPT-30B,
        released last week, is an extremely capable Apache 2 licensed open source
        language model. This repo shows how it can be run on a CPU, using the ctransformers
        Python library based on GGML. Following the instructions in the README got
        me a working MPT-30B model on my M2 MacBook Pro. The model is a 19GB download
        and it takes a few seconds to start spitting out tokens, but it works as advertised.&lt;/p&gt;\n\n\n\n</summary><category
        term=\"llms\"/><category term=\"ai\"/><category term=\"homebrewllms\"/><category
        term=\"generativeai\"/><category term=\"opensource\"/></entry><entry><title>Weeknotes:
        symbex, LLM prompt templates, a bit of a break</title><link href=\"http://simonwillison.net/2023/Jun/27/weeknotes/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-27T16:30:57+00:00</published><updated>2023-06-27T16:30:57+00:00</updated><id>http://simonwillison.net/2023/Jun/27/weeknotes/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;I had a holiday to the UK for a family wedding
        anniversary and mostly took the time off... except for building &lt;strong&gt;symbex&lt;/strong&gt;,
        which became one of those projects that kept on inspiring new features.&lt;/p&gt;\r\n&lt;p&gt;I've
        also been working on some major improvements to my &lt;a href=\"https://llm.datasette.io/\"&gt;LLM&lt;/a&gt;
        tool for working with language models from the command-line.&lt;/p&gt;\r\n&lt;h4&gt;symbex&lt;/h4&gt;\r\n&lt;p&gt;I
        introduced &lt;a href=\"https://github.com/simonw/symbex\"&gt;symbex&lt;/a&gt;
        in &lt;a href=\"https://simonwillison.net/2023/Jun/18/symbex/\"&gt;symbex:
        search Python code for functions and classes, then pipe them into a LLM&lt;/a&gt;.
        It's since grown a bunch more features across &lt;a href=\"https://github.com/simonw/symbex/releases\"&gt;12
        total releases&lt;/a&gt;.&lt;/p&gt;\r\n&lt;p&gt;&lt;code&gt;symbex&lt;/code&gt;
        is a tool for searching Python code. The initial goal was to make it quick
        to find and output the body of a specific Python function or class, such that
        you could then pipe it to &lt;a href=\"https://llm.datasette.io/\"&gt;LLM&lt;/a&gt;
        to process it with GPT-3.5 or GPT-4:&lt;/p&gt;\r\n&lt;div class=\"highlight
        highlight-source-shell\"&gt;&lt;pre&gt;symbex find_symbol_nodes \\\r\n  &lt;span
        class=\"pl-k\"&gt;|&lt;/span&gt; llm -m gpt4 --system &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;'&lt;/span&gt;Describe this code succinctly&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Output:&lt;/p&gt;\r\n&lt;blockquote&gt;\r\n&lt;p&gt;This
        code defines a function &lt;code&gt;find_symbol_nodes&lt;/code&gt; that takes
        in three arguments: code (string), filename (string), and symbols (iterable
        of strings). The function parses the given code and searches for AST nodes
        (Class, Function, AsyncFunction) that match the provided symbols. It returns
        a list of tuple pairs containing matched nodes and their corresponding class
        names or None.&lt;/p&gt;\r\n&lt;/blockquote&gt;\r\n&lt;p&gt;When piping to
        a language model token count is really important - the goal is to provide
        the shortest amount of text that gives the model enough to produce interesting
        results.&lt;/p&gt;\r\n&lt;p&gt;So... I added a &lt;code&gt;-s/--signatures&lt;/code&gt;
        option which returns just the function or class signature:&lt;/p&gt;\r\n&lt;div
        class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;symbex find_symbol_nodes
        -s&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Output:&lt;/p&gt;\r\n&lt;pre&gt;&lt;span
        class=\"pl-c\"&gt;# File: symbex/lib.py Line: 13&lt;/span&gt;\r\n&lt;span
        class=\"pl-k\"&gt;def&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;find_symbol_nodes&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;code&lt;/span&gt;: &lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;filename&lt;/span&gt;: &lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;symbols&lt;/span&gt;: &lt;span class=\"pl-v\"&gt;Iterable&lt;/span&gt;[&lt;span
        class=\"pl-s1\"&gt;str&lt;/span&gt;]) &lt;span class=\"pl-c1\"&gt;-&amp;gt;&lt;/span&gt;
        &lt;span class=\"pl-v\"&gt;List&lt;/span&gt;[&lt;span class=\"pl-v\"&gt;Tuple&lt;/span&gt;[(&lt;span
        class=\"pl-v\"&gt;AST&lt;/span&gt;, &lt;span class=\"pl-v\"&gt;Optional&lt;/span&gt;[&lt;span
        class=\"pl-s1\"&gt;str&lt;/span&gt;])]]&lt;/pre&gt;\r\n&lt;p&gt;Add &lt;code&gt;--docstrings&lt;/code&gt;
        to include the docstring. Add &lt;code&gt;-i/--imports&lt;/code&gt; for an
        import line, and &lt;code&gt;-n/--no-file&lt;/code&gt; to suppress that &lt;code&gt;#
        File&lt;/code&gt; comment - so &lt;code&gt;-in&lt;/code&gt; combines both
        of hose options:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;symbex
        find_symbol_nodes -s --docstrings -in&lt;/pre&gt;&lt;/div&gt;\r\n&lt;pre&gt;&lt;span
        class=\"pl-c\"&gt;# from symbex.lib import find_symbol_nodes&lt;/span&gt;\r\n&lt;span
        class=\"pl-k\"&gt;def&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;find_symbol_nodes&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;code&lt;/span&gt;: &lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;filename&lt;/span&gt;: &lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;symbols&lt;/span&gt;: &lt;span class=\"pl-v\"&gt;Iterable&lt;/span&gt;[&lt;span
        class=\"pl-s1\"&gt;str&lt;/span&gt;]) &lt;span class=\"pl-c1\"&gt;-&amp;gt;&lt;/span&gt;
        &lt;span class=\"pl-v\"&gt;List&lt;/span&gt;[&lt;span class=\"pl-v\"&gt;Tuple&lt;/span&gt;[(&lt;span
        class=\"pl-v\"&gt;AST&lt;/span&gt;, &lt;span class=\"pl-v\"&gt;Optional&lt;/span&gt;[&lt;span
        class=\"pl-s1\"&gt;str&lt;/span&gt;])]]\r\n    \"&lt;span class=\"pl-v\"&gt;Returns&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;ast&lt;/span&gt; &lt;span class=\"pl-v\"&gt;Nodes&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;matching&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;symbols&lt;/span&gt;\"&lt;/pre&gt;\r\n&lt;p&gt;Being
        able to see type annotations and docstrings tells you a lot about the code.
        This gave me an idea for an extra set of features: filters that could be used
        to only return symbols that were documented, or undocumented, or included
        or were missing type signatures:&lt;/p&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--async&lt;/code&gt;:
        Filter async functions&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--function&lt;/code&gt;:
        Filter functions&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--class&lt;/code&gt;:
        Filter classes&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--documented&lt;/code&gt;:
        Filter functions with docstrings&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--undocumented&lt;/code&gt;:
        Filter functions without docstrings&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--typed&lt;/code&gt;:
        Filter functions with type annotations&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--untyped&lt;/code&gt;:
        Filter functions without type annotations&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--partially-typed&lt;/code&gt;:
        Filter functions with partial type annotations&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;code&gt;--fully-typed&lt;/code&gt;:
        Filter functions with full type annotations&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;p&gt;So
        now you can use &lt;code&gt;symbex&lt;/code&gt; to get a feel for how well
        typed or documented your code is:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;&lt;span
        class=\"pl-c\"&gt;&lt;span class=\"pl-c\"&gt;#&lt;/span&gt; See all symbols
        lacking a docstring:&lt;/span&gt;\r\nsymbex -s --undocumented\r\n\r\n&lt;span
        class=\"pl-c\"&gt;&lt;span class=\"pl-c\"&gt;#&lt;/span&gt; All functions
        that are missing type annotations:&lt;/span&gt;\r\nsymbex -s --function --untyped&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;The
        &lt;a href=\"https://github.com/simonw/symbex/blob/main/README.md\"&gt;README&lt;/a&gt;
        has comprehensive documentation on everything else the tool can do.&lt;/p&gt;\r\n&lt;h4&gt;LLM
        prompt templates&lt;/h4&gt;\r\n&lt;p&gt;My &lt;a href=\"https://llm.datasette.io/\"&gt;LLM&lt;/a&gt;
        tool is shaping up in some interesting directions as well.&lt;/p&gt;\r\n&lt;p&gt;The
        big new released feature is &lt;a href=\"https://llm.datasette.io/en/stable/templates.html\"&gt;prompt
        templates&lt;/a&gt;.&lt;/p&gt;\r\n&lt;p&gt;A template is a file that looks
        like this:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-yaml\"&gt;&lt;pre&gt;&lt;span
        class=\"pl-ent\"&gt;system&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;Summarize
        this text in the voice of $voice&lt;/span&gt;\r\n&lt;span class=\"pl-ent\"&gt;model&lt;/span&gt;:
        &lt;span class=\"pl-s\"&gt;gpt-4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;This
        can be installed using &lt;code&gt;llm templates edit summary&lt;/code&gt;,
        which opens a text editor (using the &lt;code&gt;$EDITOR&lt;/code&gt; environment
        variable).&lt;/p&gt;\r\n&lt;p&gt;Once installed, you can use it like this:&lt;/p&gt;\r\n&lt;div
        class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;curl -s &lt;span
        class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;https://til.simonwillison.net/macos/imovie-slides-and-audio&lt;span
        class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-k\"&gt;|&lt;/span&gt;
        \\\r\n  strip-tags -m &lt;span class=\"pl-k\"&gt;|&lt;/span&gt; \\\r\n  llm
        -t summarize -p voice &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Extremely
        sarcastic GlaDOS&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;blockquote&gt;\r\n&lt;p&gt;Oh,
        &lt;em&gt;bravo&lt;/em&gt;, Simon. You've really outdone yourself. Apparently,
        the highlight of his day was turning an old talk into a video using iMovie.
        After a truly heart-stopping struggle with the Ken Burns effect, he finally,
        and I mean &lt;em&gt;finally&lt;/em&gt;, tuned the slide duration to match
        the audio. And then, hold your applause, he met the enormous challenge of
        publishing it on YouTube. We were all waiting with bated breath. Oh, but wouldn't
        it be exciting to note that his estimated 1.03GB video was actually a shockingly
        smaller size? I can't contain my excitement. He also used Pixelmator for a
        custom title slide, as YouTube prefers a size of 1280x720px - ground-breaking
        information, truly.&lt;/p&gt;\r\n&lt;/blockquote&gt;\r\n&lt;p&gt;The idea
        here is to make it easy to create reusable template snippets, for all sorts
        of purposes. &lt;code&gt;git diff | llm -t diff&lt;/code&gt; could generate
        a commit message, &lt;code&gt;cat file.py | llm -t explain&lt;/code&gt; could
        explain code etc.&lt;/p&gt;\r\n&lt;h4&gt;LLM plugins&lt;/h4&gt;\r\n&lt;p&gt;These
        are still baking, but this is the feature I'm most excited about. I'm adding
        plugins to LLM, inspired by &lt;a href=\"https://docs.datasette.io/en/stable/plugins.html\"&gt;plugins
        in Datasette&lt;/a&gt;.&lt;/p&gt;\r\n&lt;p&gt;I'm planning the following categories
        of plugins to start with:&lt;/p&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;Command
        plugins&lt;/strong&gt;. These will allow extra commands to be added to the
        &lt;code&gt;llm&lt;/code&gt; tool - &lt;code&gt;llm search&lt;/code&gt; or
        &lt;code&gt;llm embed&lt;/code&gt; or similar.&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;Template
        plugins&lt;/strong&gt;. Imagine being able to install extra prompt templates
        using &lt;code&gt;llm install name-of-package&lt;/code&gt;.&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;Model
        plugins&lt;/strong&gt;. I want LLM to be able to use more than just GPT-3.5
        and GPT-4. I have a branch with &lt;a href=\"https://github.com/simonw/llm/blob/ce2a322126f98a2702077eb06d0b57c8a8414d42/llm/vertex_models.py\"&gt;an
        example plugin&lt;/a&gt; that can call Google's PaLM 2 model via &lt;a href=\"https://cloud.google.com/vertex-ai\"&gt;Google
        Vertex&lt;/a&gt;, and I hope to support many other LLM families with additional
        plugins, including models that can run locally via &lt;a href=\"https://github.com/ggerganov/llama.cpp\"&gt;llama.cpp&lt;/a&gt;
        and similar.&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;Function plugins&lt;/strong&gt;.
        Once I get the new OpenAI functions mechanism working, I'd like to be able
        to install plugins that make new functions available to be executed by the
        LLM!&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;p&gt;All of this is under active development
        at the moment. I'll write more about it once I get it working.&lt;/p&gt;\r\n&lt;h4&gt;Entries
        these weeks&lt;/h4&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;&lt;a href=\"https://simonwillison.net/2023/Jun/18/symbex/\"&gt;symbex:
        search Python code for functions and classes, then pipe them into a LLM&lt;/a&gt;&lt;/li&gt;\r\n&lt;li&gt;&lt;a
        href=\"https://simonwillison.net/2023/Jun/8/gpt-tokenizers/\"&gt;Understanding
        GPT tokenizers&lt;/a&gt;&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;h4&gt;Releases these
        weeks&lt;/h4&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a href=\"https://github.com/simonw/sqlite-utils/releases/tag/3.33\"&gt;sqlite-utils
        3.33&lt;/a&gt;&lt;/strong&gt; - 2023-06-26&lt;br /&gt;Python CLI utility and
        library for manipulating SQLite databases&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;strong&gt;&lt;a
        href=\"https://github.com/simonw/datasette-render-images/releases/tag/0.4\"&gt;datasette-render-images
        0.4&lt;/a&gt;&lt;/strong&gt; - 2023-06-14&lt;br /&gt;Datasette plugin that
        renders binary blob images using data-uris&lt;/li&gt;\r\n&lt;/ul&gt;\r\n&lt;h4&gt;TIL
        these weeks&lt;/h4&gt;\r\n&lt;ul&gt;\r\n&lt;li&gt;\r\n&lt;a href=\"https://til.simonwillison.net/python/toml\"&gt;TOML
        in Python&lt;/a&gt; - 2023-06-26&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a href=\"https://til.simonwillison.net/homebrew/auto-formulas-github-actions\"&gt;Automatically
        maintaining Homebrew formulas using GitHub Actions&lt;/a&gt; - 2023-06-21&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/gpt3/picking-python-project-name-chatgpt\"&gt;Using
        ChatGPT Browse to name a Python package&lt;/a&gt; - 2023-06-18&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/macos/imovie-slides-and-audio\"&gt;Syncing
        slide images and audio in iMovie&lt;/a&gt; - 2023-06-15&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/macos/fs-usage\"&gt;Using fs_usage to
        see what files a process is using&lt;/a&gt; - 2023-06-15&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/llms/larger-context-openai-models-llm\"&gt;Running
        OpenAI's large context models using llm&lt;/a&gt; - 2023-06-13&lt;/li&gt;\r\n&lt;li&gt;\r\n&lt;a
        href=\"https://til.simonwillison.net/sql/consecutive-groups\"&gt;Consecutive
        groups in SQL using window functions&lt;/a&gt; - 2023-06-08&lt;/li&gt;\r\n&lt;/ul&gt;\n\n</summary><category
        term=\"generativeai\"/><category term=\"projects\"/><category term=\"ai\"/><category
        term=\"weeknotes\"/><category term=\"llms\"/><category term=\"symbex\"/><category
        term=\"llm\"/></entry><entry><title>Status of Python Versions</title><link
        href=\"http://simonwillison.net/2023/Jun/27/status-of-python-versions/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-27T14:01:44+00:00</published><updated>2023-06-27T14:01:44+00:00</updated><id>http://simonwillison.net/2023/Jun/27/status-of-python-versions/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://devguide.python.org/versions/\"&gt;Status
        of Python Versions&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Very clear and useful page
        showing the exact status of different Python versions. 3.7 reaches end of
        life today (no more security updates), while 3.11 will continue to be supported
        until October 2027.&lt;/p&gt;\n\n    &lt;p&gt;Via &lt;a href=\"https://twitter.com/sethmlarson/status/1673670515981004800\"&gt;@sethmlarson&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"python\"/></entry><entry><title>Quoting sametmax</title><link href=\"http://simonwillison.net/2023/Jun/23/sametmax/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-23T23:59:54+00:00</published><updated>2023-06-23T23:59:54+00:00</updated><id>http://simonwillison.net/2023/Jun/23/sametmax/#atom-everything</id><summary
        type=\"html\">\n    &lt;blockquote cite=\"https://news.ycombinator.com/item?id=36429671\"&gt;&lt;p&gt;Every
        year, some generation of engineers have to learn the concepts of &amp;quot;there
        is no silver bullet&amp;quot;, &amp;quot;use the right tech for the right
        problem&amp;quot;, &amp;quot;your are not google&amp;quot;, &amp;quot;rewriting
        a codebase every 2 years is not a good business decision&amp;quot;, &amp;quot;things
        cost money&amp;quot;.&lt;/p&gt;&lt;/blockquote&gt;&lt;p class=\"cite\"&gt;&amp;mdash;
        &lt;a href=\"https://news.ycombinator.com/item?id=36429671\"&gt;sametmax&lt;/a&gt;\n\n</summary><category
        term=\"softwareengineering\"/></entry><entry><title>Quoting Eric Urquhart</title><link
        href=\"http://simonwillison.net/2023/Jun/22/eric-urquhart/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-22T11:13:46+00:00</published><updated>2023-06-22T11:13:46+00:00</updated><id>http://simonwillison.net/2023/Jun/22/eric-urquhart/#atom-everything</id><summary
        type=\"html\">\n    &lt;blockquote cite=\"https://venturebeat.com/ai/adobe-stock-creators-arent-happy-with-firefly-the-companys-commercially-safe-gen-ai-tool/\"&gt;&lt;p&gt;Back
        then [in 2012], no one was thinking about AI. You just keep uploading your
        images [to Adobe Stock] and you get your residuals every month and life goes
        on \u2014 then all of a sudden, you find out that they trained their AI on
        your images and on everybody\u2019s images that they don\u2019t own. And they\u2019re
        calling it \u2018ethical\u2019 AI.&lt;/p&gt;&lt;/blockquote&gt;&lt;p class=\"cite\"&gt;&amp;mdash;
        &lt;a href=\"https://venturebeat.com/ai/adobe-stock-creators-arent-happy-with-firefly-the-companys-commercially-safe-gen-ai-tool/\"&gt;Eric
        Urquhart&lt;/a&gt;\n\n</summary><category term=\"ai\"/><category term=\"adobe\"/><category
        term=\"ethics\"/><category term=\"generativeai\"/></entry><entry><title>Building
        Search DSLs with Django</title><link href=\"http://simonwillison.net/2023/Jun/19/building-search-dsls-with-django/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-19T08:30:32+00:00</published><updated>2023-06-19T08:30:32+00:00</updated><id>http://simonwillison.net/2023/Jun/19/building-search-dsls-with-django/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://danlamanna.com/posts/building-search-dsls-with-django/\"&gt;Building
        Search DSLs with Django&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Neat tutorial by Dan
        Lamanna: how to build a GitHub-style search feature - supporting modifiers
        like &amp;quot;is:open author:danlamanna&amp;quot; - using PyParsing and the
        Django ORM.&lt;/p&gt;\n\n    &lt;p&gt;Via &lt;a href=\"https://lobste.rs/s/itjx6c/building_search_dsls_with_django\"&gt;Lobsters&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"search\"/><category term=\"parsing\"/><category term=\"django\"/><category
        term=\"python\"/></entry><entry><title>symbex: search Python code for functions
        and classes, then pipe them into a LLM</title><link href=\"http://simonwillison.net/2023/Jun/18/symbex/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-18T22:11:12+00:00</published><updated>2023-06-18T22:11:12+00:00</updated><id>http://simonwillison.net/2023/Jun/18/symbex/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;I just released a new Python CLI tool called
        &lt;a href=\"https://github.com/simonw/symbex\"&gt;symbex&lt;/a&gt;. It's
        a search tool, loosely inspired by &lt;a href=\"https://github.com/BurntSushi/ripgrep\"&gt;ripgrep&lt;/a&gt;,
        which lets you search Python code for functions and classes by name or wildcard,
        then see just the source code of those matching entities.&lt;/p&gt;\r\n&lt;h4&gt;Searching
        for functions and classes&lt;/h4&gt;\r\n&lt;p&gt;Here's an example of what
        it can do. Running in my &lt;code&gt;datasette/&lt;/code&gt; folder:&lt;/p&gt;\r\n&lt;div
        class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;symbex inspect_hash
        \           &lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Output:&lt;/p&gt;\r\n&lt;pre&gt;&lt;span
        class=\"pl-c\"&gt;# File: datasette/inspect.py Line: 17&lt;/span&gt;\r\n&lt;span
        class=\"pl-k\"&gt;def&lt;/span&gt; &lt;span class=\"pl-en\"&gt;inspect_hash&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;path&lt;/span&gt;):\r\n    &lt;span class=\"pl-s\"&gt;\"\"\"Calculate
        the hash of a database, efficiently.\"\"\"&lt;/span&gt;\r\n    &lt;span class=\"pl-s1\"&gt;m&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;hashlib&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;sha256&lt;/span&gt;()\r\n    &lt;span class=\"pl-k\"&gt;with&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;path&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;open&lt;/span&gt;(&lt;span
        class=\"pl-s\"&gt;\"rb\"&lt;/span&gt;) &lt;span class=\"pl-k\"&gt;as&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;fp&lt;/span&gt;:\r\n        &lt;span class=\"pl-k\"&gt;while&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;True&lt;/span&gt;:\r\n            &lt;span class=\"pl-s1\"&gt;data&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;fp&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;read&lt;/span&gt;(&lt;span class=\"pl-v\"&gt;HASH_BLOCK_SIZE&lt;/span&gt;)\r\n
        \           &lt;span class=\"pl-k\"&gt;if&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;not&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;data&lt;/span&gt;:\r\n                &lt;span
        class=\"pl-k\"&gt;break&lt;/span&gt;\r\n            &lt;span class=\"pl-s1\"&gt;m&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;update&lt;/span&gt;(&lt;span class=\"pl-s1\"&gt;data&lt;/span&gt;)\r\n\r\n
        \   &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;m&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;hexdigest&lt;/span&gt;()&lt;/pre&gt;\r\n&lt;p&gt;I gave
        it the name of a function (classes work too) and it searched all subfolders
        of the current directory, found that function and output it to my terminal.&lt;/p&gt;\r\n&lt;p&gt;Why
        is this more useful than &lt;code&gt;ripgrep&lt;/code&gt; or any of the many
        other tools that can do this?&lt;/p&gt;\r\n&lt;p&gt;I partly built this to
        have fun learning Python's &lt;a href=\"https://docs.python.org/3/library/ast.html\"&gt;ast&lt;/a&gt;
        module, but it's mainly designed to complement my &lt;a href=\"https://llm.datasette.io/\"&gt;LLM&lt;/a&gt;
        CLI tool for running large language model prompts.&lt;/p&gt;\r\n&lt;h4&gt;Code
        explanations with a large language model&lt;/h4&gt;\r\n&lt;p&gt;Check this
        out:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;symbex
        inspect_hash &lt;span class=\"pl-k\"&gt;|&lt;/span&gt; llm --system &lt;span
        class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;explain succinctly&lt;span
        class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;Output:&lt;/p&gt;\r\n&lt;blockquote&gt;\r\n&lt;p&gt;This
        function calculates the hash of a database file efficiently by reading the
        file in blocks and updating the hash object using SHA256 algorithm from the
        hashlib module. The resulting hash value is returned as a hexadecimal string.&lt;/p&gt;\r\n&lt;/blockquote&gt;\r\n&lt;p&gt;This
        is pretty cool!&lt;/p&gt;\r\n&lt;p&gt;&lt;code&gt;llm --system 'explain succinctly'&lt;/code&gt;
        runs a prompt against &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; using \"explain
        succinctly\" as the system prompt.&lt;/p&gt;\r\n&lt;p&gt;The system prompt
        says what to do, then the content piped to the tool is treated as the data
        that should be processed.&lt;/p&gt;\r\n&lt;h4&gt;Using wildcards and guessing
        what a tool does from tests&lt;/h4&gt;\r\n&lt;p&gt;That was a pretty basic
        example. Here's something more fun:&lt;/p&gt;\r\n&lt;div class=\"highlight
        highlight-source-shell\"&gt;&lt;pre&gt;symbex &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;'&lt;/span&gt;test*csv*&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;
        &lt;span class=\"pl-k\"&gt;|&lt;/span&gt; llm --system &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;'&lt;/span&gt;based on these tests guess what this tool
        does&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;I'm
        using the &lt;code&gt;test*csv*&lt;/code&gt; wildcard here to find all of
        my test functions that mention CSV in the name. There are 18 of those, with
        names like &lt;code&gt;test_table_csv_with_labels&lt;/code&gt; and &lt;code&gt;test_table_csv_cors_headers&lt;/code&gt;.&lt;/p&gt;\r\n&lt;p&gt;I
        can count the number of tokens in the test output like this (see &lt;a href=\"https://simonwillison.net/2023/May/18/cli-tools-for-llms/\"&gt;llm,
        ttok and strip-tags\u2014CLI tools for working with ChatGPT and other LLMs&lt;/a&gt;
        for an intro to my &lt;code&gt;ttok&lt;/code&gt; tool):&lt;/p&gt;\r\n&lt;div
        class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;symbex &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;'&lt;/span&gt;test_*csv*&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;
        &lt;span class=\"pl-k\"&gt;|&lt;/span&gt; ttok&lt;/pre&gt;&lt;/div&gt;\r\n&lt;pre&gt;&lt;code&gt;2682\r\n&lt;/code&gt;&lt;/pre&gt;\r\n&lt;p&gt;That's
        significantly below the 4000 token limit for &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;.&lt;/p&gt;\r\n&lt;p&gt;Here's
        what I get when I ask it \"based on these tests guess what this tool does\":&lt;/p&gt;\r\n&lt;blockquote&gt;\r\n&lt;p&gt;This
        tool seems to be a web application that provides a user interface to query
        databases and export query results in various formats, including HTML, JSON,
        and CSV. The tests verify that the tool correctly generates HTML pages with
        links to export query results in JSON or CSV format, exports tables to CSV
        format with custom options (such as whether to include labels and nullable
        labels), and performs streaming of CSV data. The tests also check that the
        tool sets appropriate HTTP headers, allows CORS, and handles errors and large
        CSV files gracefully.&lt;/p&gt;\r\n&lt;/blockquote&gt;\r\n&lt;p&gt;That's
        a good summary of what Datasette does!&lt;/p&gt;\r\n&lt;p&gt;I was worried
        that it might be cheating since ChatGPT already knows about Datasette, but
        I tried this:&lt;/p&gt;\r\n&lt;div class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;symbex
        &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;test_*csv*&lt;span
        class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-k\"&gt;|&lt;/span&gt;
        grep datasette&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;And got no results, so
        at least that keyword wasn't being leaked in the test details somehow.&lt;/p&gt;\r\n&lt;h4&gt;Refactoring
        code&lt;/h4&gt;\r\n&lt;p&gt;Let's try something a whole lot more useful:&lt;/p&gt;\r\n&lt;div
        class=\"highlight highlight-source-shell\"&gt;&lt;pre&gt;symbex Request &lt;span
        class=\"pl-k\"&gt;|&lt;/span&gt; llm --system &lt;span class=\"pl-s\"&gt;&lt;span
        class=\"pl-pds\"&gt;'&lt;/span&gt;add type hints to this&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;\r\n&lt;p&gt;This
        locates the &lt;code&gt;Request&lt;/code&gt; class in Datasette - &lt;a href=\"https://github.com/simonw/datasette/blob/dda99fc09fb0b5523948f6d481c6c051c1c7b5de/datasette/utils/asgi.py#L56-L156\"&gt;this
        one here&lt;/a&gt;, and starts adding Python type hints to it. The output
        started out like this (that code has no type hints at all at the moment):&lt;/p&gt;\r\n&lt;pre&gt;&lt;span
        class=\"pl-k\"&gt;from&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;typing&lt;/span&gt;
        &lt;span class=\"pl-k\"&gt;import&lt;/span&gt; &lt;span class=\"pl-v\"&gt;Dict&lt;/span&gt;,
        &lt;span class=\"pl-v\"&gt;Any&lt;/span&gt;, &lt;span class=\"pl-v\"&gt;Awaitable&lt;/span&gt;\r\n&lt;span
        class=\"pl-k\"&gt;from&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;http&lt;/span&gt;.&lt;span
        class=\"pl-s1\"&gt;cookies&lt;/span&gt; &lt;span class=\"pl-k\"&gt;import&lt;/span&gt;
        &lt;span class=\"pl-v\"&gt;SimpleCookie&lt;/span&gt;\r\n&lt;span class=\"pl-k\"&gt;from&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;urllib&lt;/span&gt;.&lt;span class=\"pl-s1\"&gt;parse&lt;/span&gt;
        &lt;span class=\"pl-k\"&gt;import&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;urlunparse&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;parse_qs&lt;/span&gt;, &lt;span class=\"pl-s1\"&gt;parse_qsl&lt;/span&gt;\r\n\r\n&lt;span
        class=\"pl-k\"&gt;from&lt;/span&gt; .&lt;span class=\"pl-s1\"&gt;multidict&lt;/span&gt;
        &lt;span class=\"pl-k\"&gt;import&lt;/span&gt; &lt;span class=\"pl-v\"&gt;MultiParams&lt;/span&gt;\r\n\r\n\r\n&lt;span
        class=\"pl-k\"&gt;class&lt;/span&gt; &lt;span class=\"pl-v\"&gt;Request&lt;/span&gt;:\r\n
        \   &lt;span class=\"pl-k\"&gt;def&lt;/span&gt; &lt;span class=\"pl-en\"&gt;__init__&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;self&lt;/span&gt;, &lt;span class=\"pl-s1\"&gt;scope&lt;/span&gt;:
        &lt;span class=\"pl-v\"&gt;Dict&lt;/span&gt;[&lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;,
        &lt;span class=\"pl-v\"&gt;Any&lt;/span&gt;], &lt;span class=\"pl-s1\"&gt;receive&lt;/span&gt;:
        &lt;span class=\"pl-v\"&gt;Awaitable&lt;/span&gt;) &lt;span class=\"pl-c1\"&gt;-&amp;gt;&lt;/span&gt;
        &lt;span class=\"pl-c1\"&gt;None&lt;/span&gt;:\r\n        &lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span
        class=\"pl-s1\"&gt;scope&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;scope&lt;/span&gt;\r\n        &lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span
        class=\"pl-s1\"&gt;receive&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;=&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;receive&lt;/span&gt;\r\n\r\n    &lt;span class=\"pl-k\"&gt;def&lt;/span&gt;
        &lt;span class=\"pl-en\"&gt;__repr__&lt;/span&gt;(&lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;)
        &lt;span class=\"pl-c1\"&gt;-&amp;gt;&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;:\r\n
        \       &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-s\"&gt;'&amp;lt;asgi.Request
        method=\"{}\" url=\"{}\"&amp;gt;'&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;format&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span class=\"pl-s1\"&gt;method&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span class=\"pl-s1\"&gt;url&lt;/span&gt;)\r\n\r\n
        \   &lt;span class=\"pl-en\"&gt;@&lt;span class=\"pl-s1\"&gt;property&lt;/span&gt;&lt;/span&gt;\r\n
        \   &lt;span class=\"pl-k\"&gt;def&lt;/span&gt; &lt;span class=\"pl-en\"&gt;method&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;self&lt;/span&gt;) &lt;span class=\"pl-c1\"&gt;-&amp;gt;&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;:\r\n        &lt;span class=\"pl-k\"&gt;return&lt;/span&gt;
        &lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span class=\"pl-s1\"&gt;scope&lt;/span&gt;[&lt;span
        class=\"pl-s\"&gt;\"method\"&lt;/span&gt;]\r\n\r\n    &lt;span class=\"pl-en\"&gt;@&lt;span
        class=\"pl-s1\"&gt;property&lt;/span&gt;&lt;/span&gt;\r\n    &lt;span class=\"pl-k\"&gt;def&lt;/span&gt;
        &lt;span class=\"pl-en\"&gt;url&lt;/span&gt;(&lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;)
        &lt;span class=\"pl-c1\"&gt;-&amp;gt;&lt;/span&gt; &lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;:\r\n
        \       &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-en\"&gt;urlunparse&lt;/span&gt;(\r\n
        \           (&lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span class=\"pl-s1\"&gt;scheme&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span class=\"pl-s1\"&gt;host&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span class=\"pl-s1\"&gt;path&lt;/span&gt;,
        &lt;span class=\"pl-c1\"&gt;None&lt;/span&gt;, &lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span
        class=\"pl-s1\"&gt;query_string&lt;/span&gt;, &lt;span class=\"pl-c1\"&gt;None&lt;/span&gt;)\r\n
        \       )\r\n\r\n    &lt;span class=\"pl-en\"&gt;@&lt;span class=\"pl-s1\"&gt;property&lt;/span&gt;&lt;/span&gt;\r\n
        \   &lt;span class=\"pl-k\"&gt;def&lt;/span&gt; &lt;span class=\"pl-en\"&gt;url_vars&lt;/span&gt;(&lt;span
        class=\"pl-s1\"&gt;self&lt;/span&gt;) &lt;span class=\"pl-c1\"&gt;-&amp;gt;&lt;/span&gt;
        &lt;span class=\"pl-v\"&gt;Dict&lt;/span&gt;[&lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;,
        &lt;span class=\"pl-s1\"&gt;str&lt;/span&gt;]:\r\n        &lt;span class=\"pl-k\"&gt;return&lt;/span&gt;
        (&lt;span class=\"pl-s1\"&gt;self&lt;/span&gt;.&lt;span class=\"pl-s1\"&gt;scope&lt;/span&gt;.&lt;span
        class=\"pl-en\"&gt;get&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;\"url_route\"&lt;/span&gt;)
        &lt;span class=\"pl-c1\"&gt;or&lt;/span&gt; {}).&lt;span class=\"pl-en\"&gt;get&lt;/span&gt;(&lt;span
        class=\"pl-s\"&gt;\"kwargs\"&lt;/span&gt;) &lt;span class=\"pl-c1\"&gt;or&lt;/span&gt;
        {}\r\n    \r\n    &lt;span class=\"pl-c\"&gt;# ...&lt;/span&gt;&lt;/pre&gt;\r\n&lt;p&gt;Now
        this is getting impressive! Obviously I wouldn't just check code like this
        in without a comprehensive review and likely adjusting many of the decisions
        it's made, but this is a very good starting point - especially for the tiny
        amount of effort it takes to get started.&lt;/p&gt;\r\n&lt;h4&gt;Picking a
        name for the tool&lt;/h4&gt;\r\n&lt;p&gt;The most time-consuming part of this
        project ended up being picking the name!&lt;/p&gt;\r\n&lt;p&gt;Originally
        I planned to call it &lt;code&gt;py-grep&lt;/code&gt;. I checked &lt;code&gt;https://pypi.org/project/py-grep/&lt;/code&gt;
        and it was available, so I spun up the first version of the tool and attempted
        to upload it to PyPI.&lt;/p&gt;\r\n&lt;p&gt;PyPI gave me an error, because
        the name was too similar to the existing &lt;code&gt;pygrep&lt;/code&gt; package.
        On the one hand that's totally fair, but it was annoying that I couldn't check
        for availability without attempting an upload.&lt;/p&gt;\r\n&lt;p&gt;I turned
        to ChatGPT to start brainstorming new names. I didn't use regular ChatGPT
        though: I fired up ChatGPT Browse, which could both read my README and, with
        some prompting, could learn to check if names were taken itself!&lt;/p&gt;\r\n&lt;p&gt;&lt;img
        src=\"https://static.simonwillison.net/static/2023/chatgpt-naming.jpg\" alt=\"Prompt:
        All of those names are taken, more ideas please - you can check if they are
        taken by checking for https://pypi.org/project/pyseek/ ChatGPT Finished browsing
        Clicked on: pydex \xB7 PyPI Clicked on: pyscan \xB7 PyPI Clicked on: pylocator
        \xB7 PyPI Clicked on: pyspotter \xB7 PyPI Clicked on: pylocator \xB7 PyPI
        Clicked on: pyfinder \xB7 PyPI Clicked on: PyScanner \xB7 PyPI Clicked on:
        PyLookup \xB7 PyPI Clicked on: PySpot \xB7 PyPI Clicked on: PyLookup \xB7
        PyPI Clicked on: pylocator \xB7 PyPI Clicked on: pyfinder \xB7 PyPI Clicked
        on: pylocator \xB7 PyPI Clicked on: pyfinder \xB7 PyPI Click failed Finished
        browsing It seems there was an error fetching the page. This might indicate
        that the name pysourcefinder is not taken, but it would be safer to verify
        manually.\" style=\"max-width: 100%;\" /&gt;&lt;/p&gt;\r\n&lt;p&gt;I wrote
        up the full process for this in a TIL: &lt;a href=\"https://til.simonwillison.net/gpt3/picking-python-project-name-chatgpt\"&gt;Using
        ChatGPT Browse to name a Python package&lt;/a&gt;.&lt;/p&gt;\n\n</summary><category
        term=\"python\"/><category term=\"generativeai\"/><category term=\"projects\"/><category
        term=\"chatgpt\"/><category term=\"ai\"/><category term=\"llms\"/><category
        term=\"symbex\"/></entry><entry><title>LLM 0.4</title><link href=\"http://simonwillison.net/2023/Jun/17/llm/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-17T22:58:34+00:00</published><updated>2023-06-17T22:58:34+00:00</updated><id>http://simonwillison.net/2023/Jun/17/llm/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://llm.datasette.io/en/stable/changelog.html#v0-4\"&gt;LLM
        0.4&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;I released a major update to my LLM CLI
        tool today - version 0.4, which adds conversation mode and prompt templates
        so you can store and re-use interesting prompts, plus a whole bunch of other
        large and small improvements.&lt;br&gt;&lt;br&gt;I also released 0.4.1 with
        some minor fixes and the ability to install the tool using Hombrew: brew install
        simonw/llm/llm&lt;/p&gt;\n\n\n\n</summary><category term=\"projects\"/><category
        term=\"generativeai\"/><category term=\"openai\"/><category term=\"chatgpt\"/><category
        term=\"ai\"/><category term=\"llms\"/><category term=\"releases\"/></entry><entry><title>sqlean.py:
        Python's sqlite3 with extensions</title><link href=\"http://simonwillison.net/2023/Jun/17/sqleanpy/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-17T22:42:13+00:00</published><updated>2023-06-17T22:42:13+00:00</updated><id>http://simonwillison.net/2023/Jun/17/sqleanpy/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://antonz.org/sqlean-py/\"&gt;sqlean.py:
        Python&amp;#x27;s sqlite3 with extensions&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Anton
        Zhiyanov built a new Python package which bundles a fresh, compiled copy of
        SQLite with his SQLean family of C extensions built right in. Installing it
        gets you the latest SQLite - 3.42.0 - with nearly 200 additional functions,
        including things like define() and eval(), fileio_read() and fileio_write(),
        percentile_95() and uuid4() and many more. &amp;quot;import sqlean as sqlite3&amp;quot;
        works as a drop-in replacement for the module from the standard library.&lt;/p&gt;\n\n\n\n</summary><category
        term=\"sqlite\"/><category term=\"antonzhiyanov\"/><category term=\"python\"/></entry><entry><title>When
        Zeppelins Ruled The Earth</title><link href=\"http://simonwillison.net/2023/Jun/15/when-zeppelins-ruled-the-earth/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-15T20:16:42+00:00</published><updated>2023-06-15T20:16:42+00:00</updated><id>http://simonwillison.net/2023/Jun/15/when-zeppelins-ruled-the-earth/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=omobajJmyIU\"&gt;When
        Zeppelins Ruled The Earth&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;15 years ago I put
        together a talk about the history of Zeppelins which I presented a bunch of
        different times in various different configurations. As far as I know there
        are no existing videos of it, but I found an MP3 recording today and decided
        to splice it together with the slides to create a video of the 6m47s version
        I gave at the Skillswap on Speed lightning talks event in Brighton on the
        28th October 2008.&lt;br&gt;&lt;br&gt;Notes on how I edited the video together
        using iMovie in the via link.&lt;/p&gt;\n\n    &lt;p&gt;Via &lt;a href=\"https://til.simonwillison.net/macos/imovie-slides-and-audio\"&gt;TIL:
        Syncing slide images and audio in iMovie&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"talks\"/><category term=\"zeppelins\"/></entry><entry><title>Example
        of OpenAI function calling API to extract data from LAPD newsroom articles</title><link
        href=\"http://simonwillison.net/2023/Jun/14/extracting-data/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-14T20:57:08+00:00</published><updated>2023-06-14T20:57:08+00:00</updated><id>http://simonwillison.net/2023/Jun/14/extracting-data/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://gist.github.com/kylemcdonald/dbac21de2d7855633689f5526225154c\"&gt;Example
        of OpenAI function calling API to extract data from LAPD newsroom articles&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Fascinating
        code example from Kyle McDonald. The OpenAI functions mechanism is intended
        to drive custom function calls, but I hadn&amp;#x27;t quite appreciated how
        useful it can be ignoring the function calls entirely. Kyle instead uses it
        to define a schema for data he wants to extract from a news article, then
        uses the gpt-3.5-turbo-0613 to get back that exact set of extracted data as
        JSON.&lt;/p&gt;\n\n    &lt;p&gt;Via &lt;a href=\"https://twitter.com/kcimc/status/1668789461780668416\"&gt;@kcimc&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"generativeai\"/><category term=\"openai\"/><category term=\"ai\"/><category
        term=\"datajournalism\"/><category term=\"llms\"/></entry><entry><title>Emergency
        Pod: OpenAI's new Functions API, 75% Price Drop, 4x Context Length</title><link
        href=\"http://simonwillison.net/2023/Jun/14/emergency-pod/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-14T19:23:38+00:00</published><updated>2023-06-14T19:23:38+00:00</updated><id>http://simonwillison.net/2023/Jun/14/emergency-pod/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://www.latent.space/p/function-agents\"&gt;Emergency
        Pod: OpenAI&amp;#x27;s new Functions API, 75% Price Drop, 4x Context Length&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;I
        participated in a Twitter Spaces conversation last night about the new OpenAI
        functions mechanism. The recording has now been turned into a Latent Space
        podcast, and swyx has accompanied the recording with a detailed write-up of
        the different topics we covered.&lt;/p&gt;\n\n    &lt;p&gt;Via &lt;a href=\"https://twitter.com/swyx/status/1669043021806198784\"&gt;@swyx&lt;/a&gt;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"llms\"/><category term=\"generativeai\"/><category term=\"openai\"/><category
        term=\"ai\"/><category term=\"speaking\"/><category term=\"podcasts\"/></entry><entry><title>Llama
        encoder and decoder</title><link href=\"http://simonwillison.net/2023/Jun/13/llama-encoder-and-decoder/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-13T22:37:29+00:00</published><updated>2023-06-13T22:37:29+00:00</updated><id>http://simonwillison.net/2023/Jun/13/llama-encoder-and-decoder/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://observablehq.com/@simonw/llama-encoder-and-decoder\"&gt;Llama
        encoder and decoder&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;I forked my GPT tokenizer
        Observable notebook to create a similar tool for exploring the tokenization
        scheme used by the Llama family of LLMs, using the new llama-tokenizer-js
        JavaScript library.&lt;/p&gt;\n\n\n\n</summary><category term=\"generativeai\"/><category
        term=\"llama\"/><category term=\"observable\"/><category term=\"ai\"/><category
        term=\"llms\"/></entry><entry><title>OpenAI: Function calling and other API
        updates</title><link href=\"http://simonwillison.net/2023/Jun/13/function-calling/#atom-everything\"
        rel=\"alternate\"/><published>2023-06-13T17:34:29+00:00</published><updated>2023-06-13T17:34:29+00:00</updated><id>http://simonwillison.net/2023/Jun/13/function-calling/#atom-everything</id><summary
        type=\"html\">\n    &lt;p&gt;&lt;a href=\"https://openai.com/blog/function-calling-and-other-api-updates\"&gt;OpenAI:
        Function calling and other API updates&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;Huge
        set of announcements from OpenAI today. A bunch of price reductions, but the
        things that most excite me are the new gpt-3.5-turbo-16k model which offers
        a 16,000 token context limit (4x the existing 3.5 turbo model) at a price
        of $0.003 per 1K input tokens and $0.004 per 1K output tokens - 1/10th the
        price of GPT-4 8k.&lt;br&gt;&lt;br&gt;The other big new feature: functions!
        You can now send JSON schema defining one or more functions to GPT 3.5 and
        GPT-4 - those models will then return a blob of JSON describing a function
        they want you to call (if they determine that one should be called). Your
        code executes the function and passes the results back to the model to continue
        the execution flow.&lt;br&gt;&lt;br&gt;This is effectively an implementation
        of the ReAct pattern, with models that have been fine-tuned to execute it.&lt;br&gt;&lt;br&gt;They
        acknowledge the risk of prompt injection (though not by name) in the post:
        &amp;quot;We are working to mitigate these and other risks. Developers can
        protect their applications by only consuming information from trusted tools
        and by including user confirmation steps before performing actions with real-world
        impact, such as sending an email, posting online, or making a purchase.&amp;quot;&lt;/p&gt;\n\n\n\n</summary><category
        term=\"gpt3\"/><category term=\"openai\"/><category term=\"gpt4\"/><category
        term=\"ai\"/><category term=\"llms\"/><category term=\"promptengineering\"/><category
        term=\"promptinjection\"/><category term=\"generativeai\"/><category term=\"chatgpt\"/></entry></feed>"
    headers:
      Accept-Ranges:
      - bytes
      Age:
      - '40'
      CF-Cache-Status:
      - HIT
      CF-RAY:
      - 7e82b42ddb2c232c-SJC
      Cache-Control:
      - s-maxage=120
      Connection:
      - close
      Content-Length:
      - '98033'
      Content-Type:
      - application/atom+xml; charset=utf-8
      Cross-Origin-Opener-Policy:
      - same-origin
      Date:
      - Mon, 17 Jul 2023 13:06:43 GMT
      Last-Modified:
      - Sun, 16 Jul 2023 05:55:54 GMT
      NEL:
      - '{"success_fraction":0,"report_to":"cf-nel","max_age":604800}'
      Referrer-Policy:
      - same-origin
      Report-To:
      - '{"endpoints":[{"url":"https:\/\/a.nel.cloudflare.com\/report\/v3?s=tpVLm13ETM3bZjZC9TMuJ%2BAT2q34NoIAh3taDGQCJsncYA8ZyVV6XtYfJpLe9x2aUm8f0MTKhf2QCw9QvCz5%2BfcoIDNu9LySmm28RU3jpwX7NO%2Fu8g3%2BLDkFtOPHskKyS7oF9Q%3D%3D"}],"group":"cf-nel","max_age":604800}'
      Server:
      - cloudflare
      Vary:
      - Accept-Encoding
      Via:
      - 1.1 vegur
      X-Content-Type-Options:
      - nosniff
      alt-svc:
      - h3=":443"; ma=86400
    status:
      code: 200
      message: OK
version: 1
